{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Michael E. Ramsey\n",
    "CSCI 5352\n",
    "Date Created: 11/05/18\n",
    "Last Edited: 12/05/18\n",
    "\n",
    "This is a python script to create edge prediction models with the features created by:\n",
    "\"Facebook_Feature_Single_Model.ipynb\".\n",
    "\n",
    "I implement several different machine learning models and check out aspects of all.\n",
    "\"\"\"\n",
    "\n",
    "# Get necessary libraries\n",
    "import sys\n",
    "import io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract all filenames for facebook100 dataset\n",
    "\"\"\"\n",
    "filepath = \"Facebook_Features_70/\"\n",
    "\n",
    "# Get list of filenames that contain edge information\n",
    "# Had to exclude a bunch of files that I did not need\n",
    "# Could have done this more efficiently\n",
    "files = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American75.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Amherst41.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Bowdoin47.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Brandeis99.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Bucknell39.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Caltech36.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Colgate88.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Hamilton46.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Haverford76.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Howard90.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Johns Hopkins55.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Lehigh96.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Middlebury45.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "MIT8.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Oberlin44.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Pepperdine86.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Reed98.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Rice31.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Rochester38.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Santa74.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Smith60.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Swarthmore42.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Trinity100.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "USFCA72.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Vassar85.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Vermont70.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Wellesley22.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Wesleyan43.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "William77.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n",
      "Williams40.csv\n",
      "logistic, SVM, Perceptron, Random Forest, Adaboost, Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loop through all files and compute accuracy, recall, precision\n",
    "\"\"\"\n",
    "\n",
    "# Empty array to store results\n",
    "models_running = 6\n",
    "acc = np.zeros((len(files),models_running))\n",
    "prec = np.zeros((len(files),models_running))\n",
    "rec = np.zeros((len(files),models_running))\n",
    "\n",
    "# Loop though files and calculate results\n",
    "file_num = 0\n",
    "for filename in files:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load the data and standardize\n",
    "    \"\"\"\n",
    "    # Print for tracking\n",
    "    print(filename)\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(filepath + filename)\n",
    "\n",
    "    # Delete the first column\n",
    "    data = data.drop(columns = ['Unnamed: 0'])\n",
    "    \n",
    "    # Drop rows with nas\n",
    "    #data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna()\n",
    "    #data = data[pd.notnull(data)]\n",
    "\n",
    "    # Standardize the data\n",
    "    temp = data[['same_gender', 'same_status','same_major','same_dorm','same_year','edge','label']]\n",
    "    data = data.drop(columns = ['Node_1','Node_2','same_gender','same_status','same_major','same_dorm','same_year','edge','label'])\n",
    "    names = list(data)\n",
    "    data = data.values #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    data_scaled = min_max_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data_scaled)\n",
    "    data.columns = names\n",
    "    data = data.join(temp.reset_index())\n",
    "    \n",
    "    \"\"\" \n",
    "    Separate into train/valid/test\n",
    "    \"\"\"\n",
    "    train = data[data['label'] == 'Tr']\n",
    "    valid = data[data['label'] == 'V']\n",
    "    test = data[data['label'] == 'T']\n",
    "\n",
    "    # Separate out values\n",
    "    y_train = train['edge']\n",
    "    x_train = train.drop(columns = ['edge', 'label'])\n",
    "    y_test = test['edge']\n",
    "    x_test = test.drop(columns = ['edge', 'label'])\n",
    "    y_valid = valid['edge']\n",
    "    x_valid = valid.drop(columns = ['edge', 'label'])\n",
    "    \n",
    "    \"\"\"\n",
    "    Logistic Regression Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    logistic = SGDClassifier(random_state=345, loss = 'log', max_iter = 10000, tol = .001, verbose = 0)\n",
    "    clf = logistic.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    acc[file_num][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    prec[file_num][0] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rec[file_num][0] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    print('logistic', end = \", \")\n",
    "    \n",
    "    \"\"\"\n",
    "    SVM Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    svm = SGDClassifier(random_state=345, loss = 'hinge', max_iter = 10000, tol = .001, verbose = 0)\n",
    "    clf = svm.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    acc[file_num][1] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    prec[file_num][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rec[file_num][1] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    print('SVM', end = \", \")\n",
    "    \n",
    "    \"\"\"\n",
    "    Perceptron Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    svm = SGDClassifier(random_state=345, loss = 'perceptron', max_iter = 30, tol = .001, verbose = 0)\n",
    "    clf = svm.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    acc[file_num][2] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    prec[file_num][2] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rec[file_num][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    print('Perceptron', end = \", \")\n",
    "    \n",
    "    \"\"\"\n",
    "    KNN Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    #neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "    #clf = neigh.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    #y_pred = clf.predict(x_valid)\n",
    "    #mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    #acc[file_num][3] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    #prec[file_num][3] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    #rec[file_num][3] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    #print('KNN', end = \", \")\n",
    "    \n",
    "    \"\"\"\n",
    "    Random Forest Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=345, verbose = 0)\n",
    "    clf = forest.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    acc[file_num][3] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    prec[file_num][3] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rec[file_num][3] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    print('Random Forest', end = \", \")\n",
    "    \n",
    "    \"\"\"\n",
    "    Adaboost Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    adaboy = AdaBoostClassifier(random_state=345)\n",
    "    clf = adaboy.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    acc[file_num][4] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    prec[file_num][4] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rec[file_num][4] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    print('Adaboost', end = \", \")\n",
    "    \n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and display the loss function\n",
    "    naive = BernoulliNB()\n",
    "    clf = naive.fit(x_train, y_train)\n",
    "    \n",
    "    # Construct confusion matrix\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    acc[file_num][5] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    prec[file_num][5] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rec[file_num][5] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    \n",
    "    # Print for tracking\n",
    "    print('Naive Bayes')\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    # Update the file number\n",
    "    file_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save results to data frame\n",
    "\"\"\"\n",
    "\n",
    "# Get names of files\n",
    "names = [x[:-4] for x in files]\n",
    "\n",
    "# Convert to data frame\n",
    "Accuracy = pd.DataFrame(data=acc,    # values\n",
    "            index=names,    # 1st column as index\n",
    "            columns=['Logistic Regression','SVM','Perceptron','Random Forest','Adaboost','Naive Bayes'])\n",
    "Precision = pd.DataFrame(data=prec,    # values\n",
    "            index=names,    # 1st column as index\n",
    "            columns=['Logistic Regression','SVM','Perceptron','Random Forest','Adaboost','Naive Bayes'])\n",
    "Recall = pd.DataFrame(data=rec,    # values\n",
    "            index=names,    # 1st column as index\n",
    "            columns=['Logistic Regression','SVM','Perceptron','Random Forest','Adaboost','Naive Bayes'])\n",
    "\n",
    "# Save to csv\n",
    "Accuracy.to_csv('Accuracy_70.csv')\n",
    "Precision.to_csv('Precision_70.csv')\n",
    "Recall.to_csv('Recall_70.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
