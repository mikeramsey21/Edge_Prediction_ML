{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Michael E. Ramsey\n",
    "CSCI 5352\n",
    "Date Created: 11/05/18\n",
    "Last Edited: 12/05/18\n",
    "\n",
    "This is a python script to create edge prediction models with the features created by:\n",
    "\"Facebook_Feature_Generator.ipynb\".\n",
    "\n",
    "I implement several different machine learning models and check out aspects of all.\n",
    "\"\"\"\n",
    "\n",
    "# Get necessary libraries\n",
    "import sys\n",
    "import io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract all filenames for facebook100 dataset\n",
    "\"\"\"\n",
    "filepath = \"Facebook_Features_97/\"\n",
    "\n",
    "# Get list of filenames that contain edge information\n",
    "# Had to exclude a bunch of files that I did not need\n",
    "# Could have done this more efficiently\n",
    "files = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American75.csv, Amherst41.csv, Bowdoin47.csv, Brandeis99.csv, Bucknell39.csv, Caltech36.csv, Colgate88.csv, Hamilton46.csv, Haverford76.csv, Howard90.csv, Johns Hopkins55.csv, Lehigh96.csv, Middlebury45.csv, MIT8.csv, Oberlin44.csv, Pepperdine86.csv, Reed98.csv, Rice31.csv, Rochester38.csv, Santa74.csv, Smith60.csv, Swarthmore42.csv, Trinity100.csv, USFCA72.csv, Vassar85.csv, Vermont70.csv, Wellesley22.csv, Wesleyan43.csv, William77.csv, Williams40.csv, "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loop through all files and compute accuracy, recall, precision\n",
    "\"\"\"\n",
    "\n",
    "# Empty data frame to store features\n",
    "feature_df = pd.DataFrame()\n",
    "\n",
    "# Number of models we are running\n",
    "models_running = 6\n",
    "\n",
    "# Loop though files and store edges\n",
    "for filename in files:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load the data \n",
    "    \"\"\"\n",
    "    # Print for tracking\n",
    "    print(filename, end = \", \")\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(filepath + filename)\n",
    "    \n",
    "    # Create column for month\n",
    "    data['school'] = [filename[:-4]]*len(data)\n",
    "    \n",
    "    # Sample 70% of the rows\n",
    "    data = data.sample(frac=0.7)\n",
    "\n",
    "    # Append to data frame\n",
    "    feature_df = feature_df.append(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Node_1</th>\n",
       "      <th>Node_2</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>common_neighbors</th>\n",
       "      <th>pref_attach</th>\n",
       "      <th>neighbor_sum</th>\n",
       "      <th>sorensen</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>hub_prom</th>\n",
       "      <th>...</th>\n",
       "      <th>same_status</th>\n",
       "      <th>same_major</th>\n",
       "      <th>same_dorm</th>\n",
       "      <th>same_year</th>\n",
       "      <th>local_path001</th>\n",
       "      <th>local_path01</th>\n",
       "      <th>local_path1</th>\n",
       "      <th>edge</th>\n",
       "      <th>label</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260963</th>\n",
       "      <td>260963</td>\n",
       "      <td>5849</td>\n",
       "      <td>1094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.224299</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.917</td>\n",
       "      <td>33.17</td>\n",
       "      <td>115.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748383</th>\n",
       "      <td>748383</td>\n",
       "      <td>1841</td>\n",
       "      <td>2841</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836603</th>\n",
       "      <td>836603</td>\n",
       "      <td>462</td>\n",
       "      <td>3723</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3306.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.37</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265978</th>\n",
       "      <td>265978</td>\n",
       "      <td>2211</td>\n",
       "      <td>3517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2318.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.124622</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.242</td>\n",
       "      <td>8.42</td>\n",
       "      <td>30.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769650</th>\n",
       "      <td>769650</td>\n",
       "      <td>696</td>\n",
       "      <td>3276</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Node_1  Node_2  shortest_path  common_neighbors  \\\n",
       "260963      260963    5849    1094            1.0              24.0   \n",
       "748383      748383    1841    2841            4.0               0.0   \n",
       "836603      836603     462    3723            2.0               1.0   \n",
       "265978      265978    2211    3517            1.0               6.0   \n",
       "769650      769650     696    3276            3.0               0.0   \n",
       "\n",
       "        pref_attach  neighbor_sum  sorensen  cosine_sim  hub_prom     ...      \\\n",
       "260963       2772.0         107.0  0.224299    0.455842  0.545455     ...       \n",
       "748383         12.0          13.0  0.000000    0.000000  0.000000     ...       \n",
       "836603       3306.0         125.0  0.008000    0.017392  0.026316     ...       \n",
       "265978       2318.0          99.0  0.060606    0.124622  0.157895     ...       \n",
       "769650        740.0          57.0  0.000000    0.000000  0.000000     ...       \n",
       "\n",
       "        same_status  same_major  same_dorm  same_year  local_path001  \\\n",
       "260963          0.0         0.0        0.0        0.0         24.917   \n",
       "748383          1.0         0.0        0.0        0.0          0.000   \n",
       "836603          1.0         0.0        0.0        0.0          1.037   \n",
       "265978          1.0         1.0        1.0        1.0          6.242   \n",
       "769650          0.0         0.0        0.0        0.0          0.003   \n",
       "\n",
       "        local_path01  local_path1  edge  label      school  \n",
       "260963         33.17        115.7   1.0     Tr  American75  \n",
       "748383          0.00          0.0   0.0     Tr  American75  \n",
       "836603          1.37          4.7   0.0     Tr  American75  \n",
       "265978          8.42         30.2   1.0     Tr  American75  \n",
       "769650          0.03          0.3   0.0     Tr  American75  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "View the data\n",
    "\"\"\"\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First column dropped\n",
      "NAs dropped\n",
      "Extraction complete\n",
      "Columns dropped\n",
      "Converted to numpy\n",
      "Data scaled\n",
      "Converted to pandas\n",
      "Data joined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process the data\n",
    "\"\"\"\n",
    "\n",
    "# Delete the first column\n",
    "feature_df = feature_df.drop(columns = ['Unnamed: 0'])\n",
    "print('First column dropped')\n",
    "#feature_df.head()\n",
    "\n",
    "# Drop lhn and adamic-adar\n",
    "feature_df = feature_df.drop(columns = ['lhn','adamic_adar'])\n",
    "\n",
    "# Drop rows with nas\n",
    "#data = data.replace([np.inf, -np.inf], np.nan)\n",
    "feature_df = feature_df.dropna()\n",
    "print('NAs dropped')\n",
    "#feature_df.head()\n",
    "#data = data[pd.notnull(data)]\n",
    "\n",
    "# Standardize the data\n",
    "temp = feature_df[['same_gender', 'same_status','same_major','same_dorm','same_year','edge','label', 'school']]\n",
    "print('Extraction complete')\n",
    "#temp.head()\n",
    "feature_df = feature_df.drop(columns = ['Node_1','Node_2','same_gender','same_status','same_major','same_dorm','same_year','edge','label','school'])\n",
    "print('Columns dropped')\n",
    "#feature_df.head()\n",
    "names = list(feature_df)\n",
    "feature_df = feature_df.values #returns a numpy array\n",
    "print('Converted to numpy')\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "feature_df_scaled = min_max_scaler.fit_transform(feature_df)\n",
    "print('Data scaled')\n",
    "feature_df = pd.DataFrame(feature_df_scaled)\n",
    "print('Converted to pandas')\n",
    "#feature_df.head()\n",
    "feature_df.columns = names\n",
    "#feature_df.head()\n",
    "feature_df = feature_df.join(temp.reset_index())\n",
    "print('Data joined')\n",
    "\n",
    "# Drop the index column\n",
    "feature_df = feature_df.drop(columns = ['index'])\n",
    "feature_df = feature_df.drop(columns = ['shortest_path'])\n",
    "\n",
    "# Drop shortest path column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_neighbors</th>\n",
       "      <th>pref_attach</th>\n",
       "      <th>neighbor_sum</th>\n",
       "      <th>sorensen</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>hub_prom</th>\n",
       "      <th>hub_depr</th>\n",
       "      <th>resource_all</th>\n",
       "      <th>local_cluster_sum</th>\n",
       "      <th>local_cluster_prod</th>\n",
       "      <th>...</th>\n",
       "      <th>local_path01</th>\n",
       "      <th>local_path1</th>\n",
       "      <th>same_gender</th>\n",
       "      <th>same_status</th>\n",
       "      <th>same_major</th>\n",
       "      <th>same_dorm</th>\n",
       "      <th>same_year</th>\n",
       "      <th>edge</th>\n",
       "      <th>label</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036923</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.448598</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.024593</td>\n",
       "      <td>0.327409</td>\n",
       "      <td>0.106239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026662</td>\n",
       "      <td>0.017555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.045556</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.167842</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009231</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.035926</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.124622</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>0.112320</td>\n",
       "      <td>0.012217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191924</td>\n",
       "      <td>0.034353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tr</td>\n",
       "      <td>American75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   common_neighbors  pref_attach  neighbor_sum  sorensen  cosine_sim  \\\n",
       "0          0.036923     0.001923      0.038889  0.448598    0.455842   \n",
       "1          0.000000     0.000008      0.004074  0.000000    0.000000   \n",
       "2          0.001538     0.002294      0.045556  0.016000    0.017392   \n",
       "3          0.009231     0.001608      0.035926  0.121212    0.124622   \n",
       "4          0.000000     0.000513      0.020370  0.000000    0.000000   \n",
       "\n",
       "   hub_prom  hub_depr  resource_all  local_cluster_sum  local_cluster_prod  \\\n",
       "0  0.545455  0.380952      0.024593           0.327409            0.106239   \n",
       "1  0.000000  0.000000      0.000000           0.068182            0.000000   \n",
       "2  0.026316  0.011494      0.000627           0.167842            0.027653   \n",
       "3  0.157895  0.098361      0.009088           0.112320            0.012217   \n",
       "4  0.000000  0.000000      0.000000           0.191924            0.034353   \n",
       "\n",
       "      ...      local_path01  local_path1  same_gender  same_status  \\\n",
       "0     ...          0.026662     0.017555          1.0          0.0   \n",
       "1     ...          0.000000     0.000000          1.0          1.0   \n",
       "2     ...          0.001101     0.000713          1.0          1.0   \n",
       "3     ...          0.006768     0.004582          0.0          1.0   \n",
       "4     ...          0.000024     0.000046          1.0          0.0   \n",
       "\n",
       "   same_major  same_dorm  same_year  edge  label      school  \n",
       "0         0.0        0.0        0.0   1.0     Tr  American75  \n",
       "1         0.0        0.0        0.0   0.0     Tr  American75  \n",
       "2         0.0        0.0        0.0   0.0     Tr  American75  \n",
       "3         1.0        1.0        1.0   1.0     Tr  American75  \n",
       "4         0.0        0.0        0.0   0.0     Tr  American75  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "View the data\n",
    "\"\"\"\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Separate into train/valid/test\n",
    "\"\"\"\n",
    "train = feature_df[feature_df['label'] == 'Tr']\n",
    "valid = feature_df[feature_df['label'] == 'V']\n",
    "test = feature_df[feature_df['label'] == 'T']\n",
    "\n",
    "# Separate out values\n",
    "y_train = train['edge']\n",
    "s_train = train['school']\n",
    "x_train = train.drop(columns = ['edge', 'label', 'school'])\n",
    "y_test = test['edge']\n",
    "s_test = test['school']\n",
    "x_test = test.drop(columns = ['edge', 'label', 'school'])\n",
    "y_valid = valid['edge']\n",
    "s_valid = valid['school']\n",
    "x_valid = valid.drop(columns = ['edge', 'label', 'school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 15.34, NNZs: 18, Bias: -1.466929, T: 10856732, Avg. loss: 0.268100\n",
      "Total training time: 7.62 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.31, NNZs: 18, Bias: -1.492893, T: 21713464, Avg. loss: 0.267461\n",
      "Total training time: 14.14 seconds.\n",
      "Convergence after 2 epochs took 14.14 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.898211, 0.834563, 0.955944],\n",
       "       [0.882869, 0.903487, 0.868862],\n",
       "       [0.885296, 0.896482, 0.878065],\n",
       "       [0.896816, 0.875565, 0.915808],\n",
       "       [0.903073, 0.879855, 0.923077],\n",
       "       [0.891228, 0.920266, 0.879365],\n",
       "       [0.894883, 0.886288, 0.901361],\n",
       "       [0.897772, 0.923632, 0.880734],\n",
       "       [0.858256, 0.928504, 0.821131],\n",
       "       [0.867267, 0.876367, 0.861394],\n",
       "       [0.906899, 0.867356, 0.941235],\n",
       "       [0.905079, 0.869440, 0.937790],\n",
       "       [0.898266, 0.895247, 0.901068],\n",
       "       [0.901745, 0.852622, 0.944231],\n",
       "       [0.887508, 0.865471, 0.904284],\n",
       "       [0.894011, 0.895911, 0.895578],\n",
       "       [0.880383, 0.929487, 0.845481],\n",
       "       [0.895478, 0.869245, 0.919182],\n",
       "       [0.909172, 0.873449, 0.941896],\n",
       "       [0.897059, 0.895278, 0.899044],\n",
       "       [0.884987, 0.837398, 0.927331],\n",
       "       [0.871044, 0.908484, 0.843363],\n",
       "       [0.890266, 0.912591, 0.872203],\n",
       "       [0.889180, 0.849611, 0.924741],\n",
       "       [0.891053, 0.886965, 0.894277],\n",
       "       [0.882565, 0.800060, 0.960588],\n",
       "       [0.902230, 0.883221, 0.917039],\n",
       "       [0.893933, 0.867653, 0.919425],\n",
       "       [0.902811, 0.855054, 0.944867],\n",
       "       [0.901615, 0.905812, 0.899502]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Logistic Regression Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "logistic = SGDClassifier(random_state=345, loss = 'log', max_iter = 10000, tol = .001, verbose = 1)\n",
    "clf = logistic.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix for each group\n",
    "log_res = np.zeros((len(files),3))\n",
    "file_names = [x[:-4] for x in files]\n",
    "counter = 0\n",
    "for file in file_names:\n",
    "    min_ind = min(np.where(s_valid == file)[0])\n",
    "    max_ind = max(np.where(s_valid == file)[0])\n",
    "    y_pred = clf.predict(x_valid.iloc[min_ind:max_ind])\n",
    "    mat = confusion_matrix(y_valid.iloc[min_ind:max_ind], y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    log_res[counter][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    log_res[counter][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    log_res[counter][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    counter += 1\n",
    "    \n",
    "# Display the results\n",
    "log_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "mytable = pd.DataFrame(data=log_res,    # values\n",
    "            index=file_names,    # 1st column as index\n",
    "            columns=['Accuracy','Precision','Recall'])\n",
    "mytable.to_csv('log97' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 10.86, NNZs: 18, Bias: -0.917161, T: 10856732, Avg. loss: 0.261778\n",
      "Total training time: 4.67 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.82, NNZs: 18, Bias: -0.973807, T: 21713464, Avg. loss: 0.260114\n",
      "Total training time: 9.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.85, NNZs: 18, Bias: -0.931382, T: 32570196, Avg. loss: 0.260067\n",
      "Total training time: 13.95 seconds.\n",
      "Convergence after 3 epochs took 13.95 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.902946, 0.847471, 0.952903],\n",
       "       [0.881929, 0.914695, 0.859567],\n",
       "       [0.888700, 0.916779, 0.869147],\n",
       "       [0.900124, 0.893634, 0.906667],\n",
       "       [0.903437, 0.891833, 0.913383],\n",
       "       [0.885965, 0.930233, 0.864198],\n",
       "       [0.895810, 0.900780, 0.891504],\n",
       "       [0.894416, 0.936861, 0.866037],\n",
       "       [0.855334, 0.941675, 0.809871],\n",
       "       [0.868954, 0.895150, 0.851240],\n",
       "       [0.911903, 0.884089, 0.935608],\n",
       "       [0.909564, 0.886944, 0.930464],\n",
       "       [0.901272, 0.910014, 0.894737],\n",
       "       [0.907140, 0.868792, 0.939780],\n",
       "       [0.889739, 0.883408, 0.893714],\n",
       "       [0.894956, 0.911524, 0.885199],\n",
       "       [0.881978, 0.948718, 0.836158],\n",
       "       [0.900279, 0.885820, 0.913870],\n",
       "       [0.915061, 0.892237, 0.936036],\n",
       "       [0.897059, 0.907083, 0.889802],\n",
       "       [0.889669, 0.857143, 0.918482],\n",
       "       [0.871044, 0.927550, 0.831624],\n",
       "       [0.890524, 0.930281, 0.860443],\n",
       "       [0.893543, 0.867761, 0.916895],\n",
       "       [0.893939, 0.905243, 0.885230],\n",
       "       [0.889642, 0.814981, 0.960267],\n",
       "       [0.902841, 0.905962, 0.899329],\n",
       "       [0.896444, 0.884916, 0.909206],\n",
       "       [0.907209, 0.869247, 0.940219],\n",
       "       [0.902624, 0.922345, 0.888514]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SVM Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "svm = SGDClassifier(random_state=345, loss = 'hinge', max_iter = 10000, tol = .001, verbose = 1)\n",
    "clf = svm.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix for each group\n",
    "svm_res = np.zeros((len(files),3))\n",
    "file_names = [x[:-4] for x in files]\n",
    "counter = 0\n",
    "for file in file_names:\n",
    "    min_ind = min(np.where(s_valid == file)[0])\n",
    "    max_ind = max(np.where(s_valid == file)[0])\n",
    "    y_pred = clf.predict(x_valid.iloc[min_ind:max_ind])\n",
    "    mat = confusion_matrix(y_valid.iloc[min_ind:max_ind], y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    svm_res[counter][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    svm_res[counter][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    svm_res[counter][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    counter += 1\n",
    "    \n",
    "# Display the results\n",
    "svm_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "mytable = pd.DataFrame(data=svm_res,    # values\n",
    "            index=file_names,    # 1st column as index\n",
    "            columns=['Accuracy','Precision','Recall'])\n",
    "mytable.to_csv('svm97' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.906628, 0.848788, 0.959500],\n",
       "       [0.880990, 0.931507, 0.847112],\n",
       "       [0.891423, 0.939107, 0.858380],\n",
       "       [0.913978, 0.917454, 0.912209],\n",
       "       [0.912711, 0.908893, 0.916209],\n",
       "       [0.880702, 0.943522, 0.847761],\n",
       "       [0.906748, 0.923449, 0.893242],\n",
       "       [0.889228, 0.947084, 0.851351],\n",
       "       [0.849976, 0.968956, 0.789272],\n",
       "       [0.873453, 0.938604, 0.830975],\n",
       "       [0.918726, 0.893216, 0.940724],\n",
       "       [0.917667, 0.896126, 0.937838],\n",
       "       [0.907977, 0.930318, 0.890853],\n",
       "       [0.913797, 0.878725, 0.943921],\n",
       "       [0.902486, 0.920564, 0.887585],\n",
       "       [0.896089, 0.938290, 0.867950],\n",
       "       [0.858054, 0.967949, 0.792651],\n",
       "       [0.906937, 0.901166, 0.913219],\n",
       "       [0.924340, 0.908189, 0.939494],\n",
       "       [0.903170, 0.928027, 0.884574],\n",
       "       [0.896400, 0.860627, 0.928571],\n",
       "       [0.869154, 0.959009, 0.811290],\n",
       "       [0.893106, 0.945890, 0.854323],\n",
       "       [0.916230, 0.906655, 0.925861],\n",
       "       [0.893458, 0.927369, 0.868468],\n",
       "       [0.908160, 0.850492, 0.963163],\n",
       "       [0.915063, 0.913337, 0.915588],\n",
       "       [0.903138, 0.908755, 0.901713],\n",
       "       [0.911071, 0.863441, 0.953908],\n",
       "       [0.905651, 0.935872, 0.883633]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perceptron Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "svm = SGDClassifier(random_state=345, loss = 'perceptron', max_iter = 30, tol = .001, verbose = 0)\n",
    "clf = svm.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix for each group\n",
    "pec_res = np.zeros((len(files),3))\n",
    "file_names = [x[:-4] for x in files]\n",
    "counter = 0\n",
    "for file in file_names:\n",
    "    min_ind = min(np.where(s_valid == file)[0])\n",
    "    max_ind = max(np.where(s_valid == file)[0])\n",
    "    y_pred = clf.predict(x_valid.iloc[min_ind:max_ind])\n",
    "    mat = confusion_matrix(y_valid.iloc[min_ind:max_ind], y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    pec_res[counter][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    pec_res[counter][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    pec_res[counter][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    counter += 1\n",
    "\n",
    "# Display results\n",
    "pec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "mytable = pd.DataFrame(data=pec_res,    # values\n",
    "            index=file_names,    # 1st column as index\n",
    "            columns=['Accuracy','Precision','Recall'])\n",
    "mytable.to_csv('pec97' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKNN Classifier\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "KNN Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "#neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "#clf = neigh.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix\n",
    "#y_pred = clf.predict(x_valid)\n",
    "#mat = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "#acc = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "#prec = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "#rec = mat[1][1]/(mat[1][1]+mat[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.910310, 0.865648, 0.950260],\n",
       "       [0.876292, 0.934620, 0.838079],\n",
       "       [0.887679, 0.941813, 0.850856],\n",
       "       [0.907155, 0.910472, 0.905637],\n",
       "       [0.908529, 0.916878, 0.902143],\n",
       "       [0.863158, 0.963455, 0.812325],\n",
       "       [0.898406, 0.926421, 0.876891],\n",
       "       [0.887702, 0.947685, 0.848681],\n",
       "       [0.845592, 0.964252, 0.786043],\n",
       "       [0.868954, 0.895991, 0.850679],\n",
       "       [0.920849, 0.900821, 0.937916],\n",
       "       [0.916655, 0.904735, 0.928172],\n",
       "       [0.899191, 0.927088, 0.878443],\n",
       "       [0.912879, 0.883114, 0.937929],\n",
       "       [0.897706, 0.912876, 0.885093],\n",
       "       [0.899679, 0.931227, 0.878639],\n",
       "       [0.859649, 0.964744, 0.796296],\n",
       "       [0.902756, 0.906384, 0.901404],\n",
       "       [0.923269, 0.917760, 0.928956],\n",
       "       [0.898969, 0.921173, 0.882525],\n",
       "       [0.896693, 0.896051, 0.898660],\n",
       "       [0.858290, 0.948522, 0.801773],\n",
       "       [0.887684, 0.940166, 0.849553],\n",
       "       [0.903578, 0.892826, 0.914159],\n",
       "       [0.893218, 0.928812, 0.867086],\n",
       "       [0.901385, 0.834975, 0.964828],\n",
       "       [0.908647, 0.932391, 0.889215],\n",
       "       [0.897490, 0.909577, 0.891261],\n",
       "       [0.912572, 0.884731, 0.936490],\n",
       "       [0.905399, 0.942886, 0.878208]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random Forest Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=345, verbose = 0)\n",
    "clf = forest.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix for each group\n",
    "rf_res = np.zeros((len(files),3))\n",
    "file_names = [x[:-4] for x in files]\n",
    "counter = 0\n",
    "for file in file_names:\n",
    "    min_ind = min(np.where(s_valid == file)[0])\n",
    "    max_ind = max(np.where(s_valid == file)[0])\n",
    "    y_pred = clf.predict(x_valid.iloc[min_ind:max_ind])\n",
    "    mat = confusion_matrix(y_valid.iloc[min_ind:max_ind], y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    rf_res[counter][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    rf_res[counter][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    rf_res[counter][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    counter += 1\n",
    "\n",
    "# Display results\n",
    "rf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "mytable = pd.DataFrame(data=rf_res,    # values\n",
    "            index=file_names,    # 1st column as index\n",
    "            columns=['Accuracy','Precision','Recall'])\n",
    "mytable.to_csv('rf97' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.920042, 0.884615, 0.951814],\n",
       "       [0.880363, 0.931507, 0.846154],\n",
       "       [0.886658, 0.939783, 0.850582],\n",
       "       [0.910877, 0.931828, 0.895422],\n",
       "       [0.906710, 0.911797, 0.902948],\n",
       "       [0.877193, 0.966777, 0.829060],\n",
       "       [0.898035, 0.921219, 0.880014],\n",
       "       [0.891059, 0.948286, 0.853355],\n",
       "       [0.844618, 0.954845, 0.789269],\n",
       "       [0.876125, 0.929913, 0.840172],\n",
       "       [0.923882, 0.909644, 0.935837],\n",
       "       [0.919693, 0.911621, 0.927862],\n",
       "       [0.904277, 0.931241, 0.883925],\n",
       "       [0.919307, 0.897205, 0.937711],\n",
       "       [0.909815, 0.931454, 0.892025],\n",
       "       [0.899112, 0.933086, 0.876397],\n",
       "       [0.861244, 0.961538, 0.800000],\n",
       "       [0.902292, 0.900859, 0.905026],\n",
       "       [0.922555, 0.914569, 0.930400],\n",
       "       [0.901451, 0.928789, 0.881142],\n",
       "       [0.902839, 0.887340, 0.917167],\n",
       "       [0.860652, 0.945663, 0.806504],\n",
       "       [0.891299, 0.940687, 0.854846],\n",
       "       [0.906632, 0.904927, 0.909644],\n",
       "       [0.898990, 0.936989, 0.870809],\n",
       "       [0.912677, 0.854670, 0.968549],\n",
       "       [0.909563, 0.937308, 0.887144],\n",
       "       [0.906904, 0.916153, 0.902429],\n",
       "       [0.919867, 0.897419, 0.939230],\n",
       "       [0.909939, 0.950401, 0.880278]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adaboost Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "adaboy = AdaBoostClassifier(random_state=345)\n",
    "clf = adaboy.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix for each group\n",
    "ada_res = np.zeros((len(files),3))\n",
    "file_names = [x[:-4] for x in files]\n",
    "counter = 0\n",
    "for file in file_names:\n",
    "    min_ind = min(np.where(s_valid == file)[0])\n",
    "    max_ind = max(np.where(s_valid == file)[0])\n",
    "    y_pred = clf.predict(x_valid.iloc[min_ind:max_ind])\n",
    "    mat = confusion_matrix(y_valid.iloc[min_ind:max_ind], y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    ada_res[counter][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    ada_res[counter][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    ada_res[counter][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    counter += 1\n",
    "\n",
    "# Display results\n",
    "ada_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "mytable = pd.DataFrame(data=ada_res,    # values\n",
    "            index=file_names,    # 1st column as index\n",
    "            columns=['Accuracy','Precision','Recall'])\n",
    "mytable.to_csv('ada97' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.815755, 0.980506, 0.737176],\n",
       "       [0.713122, 0.998755, 0.637014],\n",
       "       [0.726004, 0.997294, 0.647912],\n",
       "       [0.740902, 0.992608, 0.661829],\n",
       "       [0.745226, 0.990926, 0.664881],\n",
       "       [0.728070, 0.993355, 0.661504],\n",
       "       [0.736003, 0.995169, 0.654928],\n",
       "       [0.700031, 0.998196, 0.628788],\n",
       "       [0.658061, 0.996237, 0.602732],\n",
       "       [0.710349, 0.992150, 0.635254],\n",
       "       [0.811524, 0.989656, 0.729045],\n",
       "       [0.793662, 0.991392, 0.712224],\n",
       "       [0.741040, 0.993078, 0.660731],\n",
       "       [0.806474, 0.988681, 0.723340],\n",
       "       [0.757489, 0.988469, 0.674978],\n",
       "       [0.743057, 0.993680, 0.665588],\n",
       "       [0.724083, 0.983974, 0.646316],\n",
       "       [0.734128, 0.992020, 0.656510],\n",
       "       [0.794254, 0.989365, 0.713081],\n",
       "       [0.737968, 0.992003, 0.658493],\n",
       "       [0.744805, 0.993031, 0.665370],\n",
       "       [0.679263, 0.997140, 0.607433],\n",
       "       [0.716757, 0.994277, 0.637638],\n",
       "       [0.816754, 0.982714, 0.739753],\n",
       "       [0.720539, 0.994709, 0.642436],\n",
       "       [0.867058, 0.968964, 0.806508],\n",
       "       [0.769630, 0.992010, 0.685350],\n",
       "       [0.771757, 0.990547, 0.692927],\n",
       "       [0.782236, 0.991398, 0.698485],\n",
       "       [0.719223, 0.995992, 0.642742]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Naive Bayes Classifier\n",
    "\"\"\"\n",
    "\n",
    "# Create the model and display the loss function\n",
    "naive = BernoulliNB()\n",
    "clf = naive.fit(x_train, y_train)\n",
    "\n",
    "# Construct confusion matrix for each group\n",
    "nb_res = np.zeros((len(files),3))\n",
    "file_names = [x[:-4] for x in files]\n",
    "counter = 0\n",
    "for file in file_names:\n",
    "    min_ind = min(np.where(s_valid == file)[0])\n",
    "    max_ind = max(np.where(s_valid == file)[0])\n",
    "    y_pred = clf.predict(x_valid.iloc[min_ind:max_ind])\n",
    "    mat = confusion_matrix(y_valid.iloc[min_ind:max_ind], y_pred)\n",
    "\n",
    "    # Compute accuracy, precision, recall\n",
    "    nb_res[counter][0] = (mat[0][0] + mat[1][1])/np.sum(mat)\n",
    "    nb_res[counter][1] = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "    nb_res[counter][2] = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "    counter += 1\n",
    "\n",
    "# Display results\n",
    "nb_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "mytable = pd.DataFrame(data=nb_res,    # values\n",
    "            index=file_names,    # 1st column as index\n",
    "            columns=['Accuracy','Precision','Recall'])\n",
    "mytable.to_csv('nb97' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
