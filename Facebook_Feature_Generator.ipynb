{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike Ramsey\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Michael E. Ramsey\n",
    "CSCI 5352\n",
    "Date Created: 11/01/18\n",
    "Last Edited: 12/3/18\n",
    "\n",
    "This is a python script to analyze the Network of Facebook data presented in CSCI 5352\n",
    "In this file, we compute features for each edge pair and a randomly sampled set of the \n",
    "non-edge pairs. \n",
    "\n",
    "The output of this scipt is a csv, containing several \"features\" of each edge and non-edge pair.\n",
    "This .csv file will be used to create machine learning models for edge prediction.\n",
    "\"\"\"\n",
    "\n",
    "# Get necessary libraries\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy.sparse import csr_matrix\n",
    "from random import randint\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract all filenames for facebook100 dataset\n",
    "You may have to edit the data location\n",
    "\"\"\"\n",
    "\n",
    "# Get list of filenames that contain edge information\n",
    "# Had to exclude a bunch of files that I did not need\n",
    "# Could have done this more efficiently\n",
    "files = [f for f in listdir(\"../Data/facebook100txt/\") if isfile(join(\"../Data/facebook100txt/\", f)) \n",
    "         if not(f.endswith('r.txt')) if f.endswith('.txt') if not(f.endswith('readme_aaron.txt'))\n",
    "         if not(f.endswith('facebook100_readme_021011.txt'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% 1.2% 2.3% 3.5% 4.6% 5.8% 6.9% 8.1% 9.2% 10.4% 11.5% 12.7% 13.8% 15.0% 16.1% 17.3% 18.4% 19.6% 20.7% 21.9% 23.0% 24.2% 25.3% 26.5% 27.6% 28.8% 29.9% 31.1% 32.2% 33.4% 34.6% 35.7% 36.9% 38.0% 39.2% 40.3% 41.5% 42.6% 43.8% 44.9% 46.1% 47.2% 48.4% 49.5% 50.7% 51.8% 53.0% 54.1% 55.3% 56.4% 57.6% 58.7% 59.9% 61.0% 62.2% 63.3% 64.5% 65.6% 66.8% 68.0% 69.1% 70.3% 71.4% 72.6% 73.7% 74.9% 76.0% 77.2% 78.3% 79.5% 80.6% 81.8% 82.9% 84.1% 85.2% 86.4% 87.5% 88.7% 89.8% 91.0% 92.1% 93.3% 94.4% 95.6% 96.7% 97.9% 99.0% American75\n",
      "0.0% 2.8% 5.6% 8.3% 11.1% 13.9% 16.7% 19.4% 22.2% 25.0% 27.8% 30.5% 33.3% 36.1% 38.9% 41.6% 44.4% 47.2% 50.0% 52.7% 55.5% 58.3% 61.1% 63.8% 66.6% 69.4% 72.2% 74.9% 77.7% 80.5% 83.3% 86.0% 88.8% 91.6% 94.4% 97.1% 99.9% Amherst41\n",
      "0.0% 3.0% 6.0% 9.0% 12.0% 14.9% 17.9% 20.9% 23.9% 26.9% 29.9% 32.9% 35.9% 38.9% 41.9% 44.8% 47.8% 50.8% 53.8% 56.8% 59.8% 62.8% 65.8% 68.8% 71.7% 74.7% 77.7% 80.7% 83.7% 86.7% 89.7% 92.7% 95.7% 98.6% Bowdoin47\n",
      "0.0% 1.8% 3.7% 5.5% 7.3% 9.1% 11.0% 12.8% 14.6% 16.4% 18.3% 20.1% 21.9% 23.7% 25.6% 27.4% 29.2% 31.0% 32.9% 34.7% 36.5% 38.3% 40.2% 42.0% 43.8% 45.6% 47.5% 49.3% 51.1% 52.9% 54.8% 56.6% 58.4% 60.3% 62.1% 63.9% 65.7% 67.6% 69.4% 71.2% 73.0% 74.9% 76.7% 78.5% 80.3% 82.2% 84.0% 85.8% 87.6% 89.5% 91.3% 93.1% 94.9% 96.8% 98.6% Brandeis99\n",
      "0.0% 1.6% 3.2% 4.7% 6.3% 7.9% 9.5% 11.1% 12.7% 14.2% 15.8% 17.4% 19.0% 20.6% 22.2% 23.7% 25.3% 26.9% 28.5% 30.1% 31.7% 33.2% 34.8% 36.4% 38.0% 39.6% 41.2% 42.7% 44.3% 45.9% 47.5% 49.1% 50.6% 52.2% 53.8% 55.4% 57.0% 58.6% 60.1% 61.7% 63.3% 64.9% 66.5% 68.1% 69.6% 71.2% 72.8% 74.4% 76.0% 77.6% 79.1% 80.7% 82.3% 83.9% 85.5% 87.1% 88.6% 90.2% 91.8% 93.4% 95.0% 96.5% 98.1% 99.7% Bucknell39\n",
      "0.0% 15.2% 30.5% 45.7% 61.0% 76.2% 91.4% Caltech36\n",
      "0.0% 1.0% 2.0% 3.0% 4.0% 5.0% 6.0% 7.0% 8.0% 9.0% 10.0% 11.0% 12.0% 13.0% 14.0% 15.0% 16.0% 17.1% 18.1% 19.1% 20.1% 21.1% 22.1% 23.1% 24.1% 25.1% 26.1% 27.1% 28.1% 29.1% 30.1% 31.1% 32.1% 33.1% 34.1% 35.1% 36.1% 37.1% 38.1% 39.1% 40.1% 41.1% 42.1% 43.1% 44.1% 45.1% 46.1% 47.1% 48.1% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike Ramsey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:146: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Mike Ramsey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:149: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Mike Ramsey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:155: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.2% 50.2% 51.2% 52.2% 53.2% 54.2% 55.2% 56.2% 57.2% 58.2% 59.2% 60.2% 61.2% 62.2% 63.2% 64.2% 65.2% 66.2% 67.2% 68.2% 69.2% 70.2% 71.2% 72.2% 73.2% 74.2% 75.2% 76.2% 77.2% 78.2% 79.2% 80.2% 81.3% 82.3% 83.3% 84.3% 85.3% 86.3% 87.3% 88.3% 89.3% 90.3% 91.3% 92.3% 93.3% 94.3% 95.3% 96.3% 97.3% 98.3% 99.3% Carnegie49\n",
      "0.0% 1.6% 3.2% 4.9% 6.5% 8.1% 9.7% 11.4% 13.0% 14.6% 16.2% 17.9% 19.5% 21.1% 22.7% 24.4% 26.0% 27.6% 29.2% 30.8% 32.5% 34.1% 35.7% 37.3% 39.0% 40.6% 42.2% 43.8% 45.5% 47.1% 48.7% 50.3% 52.0% 53.6% 55.2% 56.8% 58.4% 60.1% 61.7% 63.3% 64.9% 66.6% 68.2% 69.8% 71.4% 73.1% 74.7% 76.3% 77.9% 79.6% 81.2% 82.8% 84.4% 86.0% 87.7% 89.3% 90.9% 92.5% 94.2% 95.8% 97.4% 99.0% Colgate88\n",
      "0.0% 2.6% 5.2% 7.9% 10.5% 13.1% 15.7% 18.3% 21.0% 23.6% 26.2% 28.8% 31.4% 34.1% 36.7% 39.3% 41.9% 44.5% 47.2% 49.8% 52.4% 55.0% 57.6% 60.3% 62.9% 65.5% 68.1% 70.7% 73.4% 76.0% 78.6% 81.2% 83.8% 86.5% 89.1% 91.7% 94.3% 96.9% 99.6% Hamilton46\n",
      "0.0% 4.3% 8.5% 12.8% 17.1% 21.3% 25.6% 29.9% 34.1% 38.4% 42.7% 46.9% 51.2% 55.5% 59.8% 64.0% 68.3% 72.6% 76.8% 81.1% 85.4% 89.6% 93.9% 98.2% Haverford76\n",
      "0.0% 1.2% 2.5% 3.7% 4.9% 6.1% 7.4% 8.6% 9.8% 11.1% 12.3% 13.5% 14.7% 16.0% 17.2% 18.4% 19.7% 20.9% 22.1% 23.3% 24.6% 25.8% 27.0% 28.3% 29.5% 30.7% 31.9% 33.2% 34.4% 35.6% 36.9% 38.1% 39.3% 40.5% 41.8% 43.0% 44.2% 45.5% 46.7% 47.9% 49.1% 50.4% 51.6% 52.8% 54.1% 55.3% 56.5% 57.7% 59.0% 60.2% 61.4% 62.7% 63.9% 65.1% 66.3% 67.6% 68.8% 70.0% 71.3% 72.5% 73.7% 74.9% 76.2% 77.4% 78.6% 79.9% 81.1% 82.3% 83.5% 84.8% 86.0% 87.2% 88.5% 89.7% 90.9% 92.1% 93.4% 94.6% 95.8% 97.1% 98.3% 99.5% Howard90\n",
      "0.0% 1.3% 2.7% 4.0% 5.4% 6.7% 8.1% 9.4% 10.8% 12.1% 13.4% 14.8% 16.1% 17.5% 18.8% 20.2% 21.5% 22.9% 24.2% 25.5% 26.9% 28.2% 29.6% 30.9% 32.3% 33.6% 35.0% 36.3% 37.6% 39.0% 40.3% 41.7% 43.0% 44.4% 45.7% 47.1% 48.4% 49.8% 51.1% 52.4% 53.8% 55.1% 56.5% 57.8% 59.2% 60.5% 61.9% 63.2% 64.5% 65.9% 67.2% 68.6% 69.9% 71.3% 72.6% 74.0% 75.3% 76.6% 78.0% 79.3% 80.7% 82.0% 83.4% 84.7% 86.1% 87.4% 88.7% 90.1% 91.4% 92.8% 94.1% 95.5% 96.8% 98.2% 99.5% Johns Hopkins55\n",
      "0.0% 1.3% 2.5% 3.8% 5.1% 6.3% 7.6% 8.9% 10.1% 11.4% 12.7% 13.9% 15.2% 16.5% 17.7% 19.0% 20.2% 21.5% 22.8% 24.0% 25.3% 26.6% 27.8% 29.1% 30.4% 31.6% 32.9% 34.2% 35.4% 36.7% 38.0% 39.2% 40.5% 41.8% 43.0% 44.3% 45.6% 46.8% 48.1% 49.4% 50.6% 51.9% 53.1% 54.4% 55.7% 56.9% 58.2% 59.5% 60.7% 62.0% 63.3% 64.5% 65.8% 67.1% 68.3% 69.6% 70.9% 72.1% 73.4% 74.7% 75.9% 77.2% 78.5% 79.7% 81.0% 82.3% 83.5% 84.8% 86.0% 87.3% 88.6% 89.8% 91.1% 92.4% 93.6% 94.9% 96.2% 97.4% 98.7% 100.0% Lehigh96\n",
      "0.0% 3.1% 6.1% 9.2% 12.2% 15.3% 18.4% 21.4% 24.5% 27.6% 30.6% 33.7% 36.7% 39.8% 42.9% 45.9% 49.0% 52.1% 55.1% 58.2% 61.2% 64.3% 67.4% 70.4% 73.5% 76.5% 79.6% 82.7% 85.7% 88.8% 91.9% 94.9% 98.0% Mich67\n",
      "0.0% 2.0% 4.0% 6.1% 8.1% 10.1% 12.1% 14.1% 16.2% 18.2% 20.2% 22.2% 24.3% 26.3% 28.3% 30.3% 32.3% 34.4% 36.4% 38.4% 40.4% 42.4% 44.5% 46.5% 48.5% 50.5% 52.5% 54.6% 56.6% 58.6% 60.6% 62.7% 64.7% 66.7% 68.7% 70.7% 72.8% 74.8% 76.8% 78.8% 80.8% 82.9% 84.9% 86.9% 88.9% 90.9% 93.0% 95.0% 97.0% 99.0% Middlebury45\n",
      "0.0% 1.0% 2.0% 3.0% 4.0% 5.0% 6.0% 7.0% 8.0% 9.0% 10.0% 11.0% 12.0% 13.0% 14.0% 15.0% 16.0% 17.0% 18.0% 19.0% 20.0% 21.0% 22.0% 23.0% 24.0% 25.0% 26.0% 26.9% 27.9% 28.9% 29.9% 30.9% 31.9% 32.9% 33.9% 34.9% 35.9% 36.9% 37.9% 38.9% 39.9% 40.9% 41.9% 42.9% 43.9% 44.9% 45.9% 46.9% 47.9% 48.9% 49.9% 50.9% 51.9% 52.9% 53.9% 54.9% 55.9% 56.9% 57.9% 58.9% 59.9% 60.9% 61.9% 62.9% 63.9% 64.9% 65.9% 66.9% 67.9% 68.9% 69.9% 70.9% 71.9% 72.9% 73.9% 74.9% 75.9% 76.9% 77.9% 78.9% 79.9% 80.8% 81.8% 82.8% 83.8% 84.8% 85.8% 86.8% 87.8% 88.8% 89.8% 90.8% 91.8% 92.8% 93.8% 94.8% 95.8% 96.8% 97.8% 98.8% 99.8% MIT8\n",
      "0.0% 2.8% 5.6% 8.4% 11.2% 14.0% 16.8% 19.6% 22.4% 25.2% 28.0% 30.7% 33.5% 36.3% 39.1% 41.9% 44.7% 47.5% 50.3% 53.1% 55.9% 58.7% 61.5% 64.3% 67.1% 69.9% 72.7% 75.5% 78.3% 81.1% 83.9% 86.7% 89.5% 92.2% 95.0% 97.8% Oberlin44\n",
      "0.0% 1.7% 3.3% 5.0% 6.6% 8.3% 9.9% 11.6% 13.2% 14.9% 16.6% 18.2% 19.9% 21.5% 23.2% 24.8% 26.5% 28.2% 29.8% 31.5% 33.1% 34.8% 36.4% 38.1% 39.7% 41.4% 43.1% 44.7% 46.4% 48.0% 49.7% 51.3% 53.0% 54.6% 56.3% 58.0% 59.6% 61.3% 62.9% 64.6% 66.2% 67.9% 69.5% 71.2% 72.9% 74.5% 76.2% 77.8% 79.5% 81.1% 82.8% 84.5% 86.1% 87.8% 89.4% 91.1% 92.7% 94.4% 96.0% 97.7% 99.4% Pepperdine86\n",
      "0.0% 13.4% 26.9% 40.3% 53.7% 67.2% 80.6% 94.1% Reed98\n",
      "0.0% 1.4% 2.7% 4.1% 5.4% 6.8% 8.2% 9.5% 10.9% 12.2% 13.6% 15.0% 16.3% 17.7% 19.1% 20.4% 21.8% 23.1% 24.5% 25.9% 27.2% 28.6% 29.9% 31.3% 32.7% 34.0% 35.4% 36.7% 38.1% 39.5% 40.8% 42.2% 43.6% 44.9% 46.3% 47.6% 49.0% 50.4% 51.7% 53.1% 54.4% 55.8% 57.2% 58.5% 59.9% 61.2% 62.6% 64.0% 65.3% 66.7% 68.0% 69.4% 70.8% 72.1% 73.5% 74.9% 76.2% 77.6% 78.9% 80.3% 81.7% 83.0% 84.4% 85.7% 87.1% 88.5% 89.8% 91.2% 92.5% 93.9% 95.3% 96.6% 98.0% 99.4% Rice31\n",
      "0.0% 1.6% 3.1% 4.7% 6.2% 7.8% 9.3% 10.9% 12.4% 14.0% 15.6% 17.1% 18.7% 20.2% 21.8% 23.3% 24.9% 26.4% 28.0% 29.5% 31.1% 32.7% 34.2% 35.8% 37.3% 38.9% 40.4% 42.0% 43.5% 45.1% 46.7% 48.2% 49.8% 51.3% 52.9% 54.4% 56.0% 57.5% 59.1% 60.6% 62.2% 63.8% 65.3% 66.9% 68.4% 70.0% 71.5% 73.1% 74.6% 76.2% 77.8% 79.3% 80.9% 82.4% 84.0% 85.5% 87.1% 88.6% 90.2% 91.7% 93.3% 94.9% 96.4% 98.0% 99.5% Rochester38\n",
      "0.0% 1.7% 3.3% 5.0% 6.6% 8.3% 9.9% 11.6% 13.3% 14.9% 16.6% 18.2% 19.9% 21.6% 23.2% 24.9% 26.5% 28.2% 29.8% 31.5% 33.2% 34.8% 36.5% 38.1% 39.8% 41.4% 43.1% 44.8% 46.4% 48.1% 49.7% 51.4% 53.0% 54.7% 56.4% 58.0% 59.7% 61.3% 63.0% 64.7% 66.3% 68.0% 69.6% 71.3% 72.9% 74.6% 76.3% 77.9% 79.6% 81.2% 82.9% 84.5% 86.2% 87.9% 89.5% 91.2% 92.8% 94.5% 96.1% 97.8% 99.5% Santa74\n",
      "0.0% 7.6% 15.3% 22.9% 30.5% 38.2% 45.8% 53.5% 61.1% 68.7% 76.4% 84.0% 91.6% 99.3% Simmons81\n",
      "0.0% 2.6% 5.2% 7.8% 10.4% 12.9% 15.5% 18.1% 20.7% 23.3% 25.9% 28.5% 31.1% 33.7% 36.2% 38.8% 41.4% 44.0% 46.6% 49.2% 51.8% 54.4% 57.0% 59.5% 62.1% 64.7% 67.3% 69.9% 72.5% 75.1% 77.7% 80.2% 82.8% 85.4% 88.0% 90.6% 93.2% 95.8% 98.4% Smith60\n",
      "0.0% 4.1% 8.3% 12.4% 16.6% 20.7% 24.9% 29.0% 33.2% 37.3% 41.5% 45.6% 49.7% 53.9% 58.0% 62.2% 66.3% 70.5% 74.6% 78.8% 82.9% 87.1% 91.2% 95.4% 99.5% Swarthmore42\n",
      "0.0% 2.3% 4.5% 6.8% 9.0% 11.3% 13.5% 15.8% 18.0% 20.3% 22.5% 24.8% 27.0% 29.3% 31.5% 33.8% 36.0% 38.3% 40.5% 42.8% 45.1% 47.3% 49.6% 51.8% 54.1% 56.3% 58.6% 60.8% 63.1% 65.3% 67.6% 69.8% 72.1% 74.3% 76.6% 78.8% 81.1% 83.4% 85.6% 87.9% 90.1% 92.4% 94.6% 96.9% 99.1% Trinity100\n",
      "0.0% 1.0% 2.0% 3.0% 4.0% 5.0% 6.0% 7.0% 8.0% 9.0% 10.0% 11.0% 12.0% 13.1% 14.1% 15.1% 16.1% 17.1% 18.1% 19.1% 20.1% 21.1% 22.1% 23.1% 24.1% 25.1% 26.1% 27.1% 28.1% 29.1% 30.1% 31.1% 32.1% 33.1% 34.1% 35.1% 36.1% 37.1% 38.2% 39.2% 40.2% 41.2% 42.2% 43.2% 44.2% 45.2% 46.2% 47.2% 48.2% 49.2% 50.2% 51.2% 52.2% 53.2% 54.2% 55.2% 56.2% 57.2% 58.2% 59.2% 60.2% 61.2% 62.2% 63.3% 64.3% 65.3% 66.3% 67.3% 68.3% 69.3% 70.3% 71.3% 72.3% 73.3% 74.3% 75.3% 76.3% 77.3% 78.3% 79.3% 80.3% 81.3% 82.3% 83.3% 84.3% 85.3% 86.3% 87.3% 88.4% 89.4% 90.4% 91.4% 92.4% 93.4% 94.4% 95.4% 96.4% 97.4% 98.4% 99.4% Tufts18\n",
      "0.0% 1.2% 2.4% 3.6% 4.8% 6.0% 7.2% 8.4% 9.6% 10.8% 12.0% 13.2% 14.5% 15.7% 16.9% 18.1% 19.3% 20.5% 21.7% 22.9% 24.1% 25.3% 26.5% 27.7% 28.9% 30.1% 31.3% 32.5% 33.7% 34.9% 36.1% 37.3% 38.5% 39.7% 41.0% 42.2% 43.4% 44.6% 45.8% 47.0% 48.2% 49.4% 50.6% 51.8% 53.0% 54.2% 55.4% 56.6% 57.8% 59.0% 60.2% 61.4% 62.6% 63.8% 65.0% 66.2% 67.4% 68.7% 69.9% 71.1% 72.3% 73.5% 74.7% 75.9% 77.1% 78.3% 79.5% 80.7% 81.9% 83.1% 84.3% 85.5% 86.7% 87.9% 89.1% 90.3% 91.5% 92.7% 93.9% 95.2% 96.4% 97.6% 98.8% 100.0% UChicago30\n",
      "0.0% 3.8% 7.7% 11.5% 15.4% 19.2% 23.1% 26.9% 30.8% 34.6% 38.5% 42.3% 46.2% 50.0% 53.9% 57.7% 61.6% 65.4% 69.3% 73.1% 77.0% 80.8% 84.7% 88.5% 92.4% 96.2% USFCA72\n",
      "0.0% 2.1% 4.2% 6.3% 8.4% 10.6% 12.7% 14.8% 16.9% 19.0% 21.1% 23.2% 25.3% 27.5% 29.6% 31.7% 33.8% 35.9% 38.0% 40.1% 42.2% 44.4% 46.5% 48.6% 50.7% 52.8% 54.9% 57.0% 59.1% 61.2% 63.4% 65.5% 67.6% 69.7% 71.8% 73.9% 76.0% 78.1% 80.3% 82.4% 84.5% 86.6% 88.7% 90.8% 92.9% 95.0% 97.1% 99.3% Vassar85\n",
      "0.0% 1.3% 2.6% 3.9% 5.2% 6.5% 7.9% 9.2% 10.5% 11.8% 13.1% 14.4% 15.7% 17.0% 18.3% 19.6% 21.0% 22.3% 23.6% 24.9% 26.2% 27.5% 28.8% 30.1% 31.4% 32.7% 34.1% 35.4% 36.7% 38.0% 39.3% 40.6% 41.9% 43.2% 44.5% 45.8% 47.1% 48.5% 49.8% 51.1% 52.4% 53.7% 55.0% 56.3% 57.6% 58.9% 60.2% 61.6% 62.9% 64.2% 65.5% 66.8% 68.1% 69.4% 70.7% 72.0% 73.3% 74.6% 76.0% 77.3% 78.6% 79.9% 81.2% 82.5% 83.8% 85.1% 86.4% 87.7% 89.1% 90.4% 91.7% 93.0% 94.3% 95.6% 96.9% 98.2% 99.5% Vermont70\n",
      "0.0% 2.6% 5.3% 7.9% 10.6% 13.2% 15.9% 18.5% 21.2% 23.8% 26.5% 29.1% 31.8% 34.4% 37.1% 39.7% 42.4% 45.0% 47.7% 50.3% 53.0% 55.6% 58.3% 60.9% 63.6% 66.2% 68.9% 71.5% 74.2% 76.8% 79.5% 82.1% 84.8% 87.4% 90.1% 92.7% 95.4% 98.0% Wellesley22\n",
      "0.0% 1.8% 3.6% 5.5% 7.3% 9.1% 10.9% 12.7% 14.6% 16.4% 18.2% 20.0% 21.9% 23.7% 25.5% 27.3% 29.1% 31.0% 32.8% 34.6% 36.4% 38.2% 40.1% 41.9% 43.7% 45.5% 47.4% 49.2% 51.0% 52.8% 54.6% 56.5% 58.3% 60.1% 61.9% 63.7% 65.6% 67.4% 69.2% 71.0% 72.9% 74.7% 76.5% 78.3% 80.1% 82.0% 83.8% 85.6% 87.4% 89.2% 91.1% 92.9% 94.7% 96.5% 98.3% Wesleyan43\n",
      "0.0% 0.9% 1.9% 2.8% 3.8% 4.7% 5.6% 6.6% 7.5% 8.5% 9.4% 10.4% 11.3% 12.2% 13.2% 14.1% 15.1% 16.0% 16.9% 17.9% 18.8% 19.8% 20.7% 21.7% 22.6% 23.5% 24.5% 25.4% 26.4% 27.3% 28.2% 29.2% 30.1% 31.1% 32.0% 33.0% 33.9% 34.8% 35.8% 36.7% 37.7% 38.6% 39.5% 40.5% 41.4% 42.4% 43.3% 44.3% 45.2% 46.1% 47.1% 48.0% 49.0% 49.9% 50.8% 51.8% 52.7% 53.7% 54.6% 55.6% 56.5% 57.4% 58.4% 59.3% 60.3% 61.2% 62.1% 63.1% 64.0% 65.0% 65.9% 66.9% 67.8% 68.7% 69.7% 70.6% 71.6% 72.5% 73.4% 74.4% 75.3% 76.3% 77.2% 78.2% 79.1% 80.0% 81.0% 81.9% 82.9% 83.8% 84.7% 85.7% 86.6% 87.6% 88.5% 89.5% 90.4% 91.3% 92.3% 93.2% 94.2% 95.1% 96.0% 97.0% 97.9% 98.9% 99.8% William77\n",
      "0.0% 2.2% 4.5% 6.7% 8.9% 11.2% 13.4% 15.6% 17.8% 20.1% 22.3% 24.5% 26.8% 29.0% 31.2% 33.5% 35.7% 37.9% 40.1% 42.4% 44.6% 46.8% 49.1% 51.3% 53.5% 55.8% 58.0% 60.2% 62.4% 64.7% 66.9% 69.1% 71.4% 73.6% 75.8% 78.1% 80.3% 82.5% 84.7% 87.0% 89.2% 91.4% 93.7% 95.9% 98.1% Williams40\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loop through all files and create a feature list.\n",
    "\"\"\"\n",
    "for filename in files:\n",
    "    \n",
    "    # Remove .txt extension\n",
    "    filename = filename[:-4]\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract the nodes and edges.\n",
    "    \"\"\"\n",
    "    # Create empty list to store connections\n",
    "    edge_list = []\n",
    "\n",
    "    # Open file and read lines\n",
    "    f = open(\"../Data/facebook100txt/\" + filename + '.txt')\n",
    "    f1 = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # Loop through lines and record edge\n",
    "    for x in f1:\n",
    "        temp = x[:-1].replace(\"\\t\",\" \")\n",
    "        edge_list.append(tuple(map(int, temp.split())))\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract Node Attributes\n",
    "    \"\"\"\n",
    "    node_attr = pd.read_table(\"../Data/facebook100txt/\" + filename + '_attr.txt')\n",
    "    \n",
    "    # Create node list\n",
    "    node_list = list(range(1,node_attr.shape[0] + 1))\n",
    "    \n",
    "    # Create list of not_edges - adjust for proportion of edges that exist in the network\n",
    "    total_val = int(np.round(len(edge_list)*(1+len(edge_list)/(len(node_list)-1)**2)))\n",
    "    not_edges = [(randint(1, len(node_list)), randint(1, len(node_list))) for _ in range(total_val)]\n",
    "    not_edges = list(set(not_edges) - set(edge_list))\n",
    "    \n",
    "    # Create label vectors\n",
    "    y_edges = np.ones((len(edge_list),1))\n",
    "    y_not_edges = np.zeros((len(not_edges),1))\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform train/valid/test split\n",
    "    We will use 95/2.5/2.5\n",
    "    \"\"\"\n",
    "    edge_train, edge_test, y_train, y_test = train_test_split(edge_list, y_edges, test_size = .025, random_state = 345)\n",
    "    not_edge_train, not_edge_test, y_not_train, y_not_test = train_test_split(not_edges, y_not_edges, test_size = .025, random_state = 345)\n",
    "\n",
    "    edge_valid, edge_test, y_valid, y_test = train_test_split(edge_test, y_test, test_size = .5, random_state = 345)\n",
    "    not_edge_valid, not_edge_test, y_not_valid, y_not_test = train_test_split(not_edge_test, y_not_test, test_size = .5, random_state = 345)\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the network\n",
    "    \"\"\"\n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(edge_train)\n",
    "\n",
    "    # Compute clustering coefficient for each ndoe\n",
    "    cluster_coeff = nx.clustering(G)\n",
    "    \n",
    "    # Construct adjacency matrices and multiples\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    Asquared = A.dot(A)\n",
    "    Acubed = Asquared.dot(A)\n",
    "\n",
    "    # Construct row sum sum diagonal matrix\n",
    "    #D = (A.todense().sum(axis = 1)).flatten()\n",
    "    #D = csr_matrix(np.diag(np.array(D)[0]))\n",
    "    #Dinv = np.diag(1/np.diag(D.todense()))\n",
    "\n",
    "    # Coompute pseudo-inverse\n",
    "    #L = D-A\n",
    "    #Lstar = np.linalg.pinv(L.todense())\n",
    "\n",
    "    # Compute rooted pagerank\n",
    "    #RPR001 = (1-.001)*np.linalg.inv(np.identity(A.shape[0]) - .001*(Dinv.dot(A.todense())))\n",
    "    #RPR01 = (1-.01)*np.linalg.inv(np.identity(A.shape[0]) - .01*(Dinv.dot(A.todense())))\n",
    "    #RPR1 = (1-.1)*np.linalg.inv(np.identity(A.shape[0]) - .1*(Dinv.dot(A.todense())))\n",
    "    \n",
    "    \"\"\"\n",
    "    Concatenate lists\n",
    "    \"\"\"\n",
    "    edge_list = edge_train + not_edge_train + edge_test + not_edge_test + edge_valid + not_edge_valid\n",
    "    y = np.concatenate((y_train, y_not_train, y_test, y_not_test, y_valid, y_not_valid))\n",
    "    label = ['Tr']*(len(edge_train)+len(not_edge_train)) + ['T']*(len(edge_test)+len(not_edge_test)) + ['V']*(len(edge_valid)+len(not_edge_valid))\n",
    "    \n",
    "    \"\"\"\n",
    "    Feature Construction\n",
    "    \"\"\"\n",
    "    # Initialize numpy arrays to store features\n",
    "    shortest_path = np.zeros((len(edge_list),1))\n",
    "    common_neighbors = np.zeros((len(edge_list),1))\n",
    "    pref_attach = np.zeros((len(edge_list),1))\n",
    "    neighbor_sum = np.zeros((len(edge_list),1))\n",
    "    local_cluster_sum = np.zeros((len(edge_list),1))\n",
    "    local_cluster_prod = np.zeros((len(edge_list),1))\n",
    "    jaccard_coeff = np.zeros((len(edge_list),1))\n",
    "    adamic_adar = np.zeros((len(edge_list),1))\n",
    "    same_gender = np.zeros((len(edge_list),1))\n",
    "    same_status = np.zeros((len(edge_list),1))\n",
    "    same_major = np.zeros((len(edge_list),1))\n",
    "    same_dorm = np.zeros((len(edge_list),1))\n",
    "    same_year = np.zeros((len(edge_list),1))\n",
    "    sorensen = np.zeros((len(edge_list),1))\n",
    "    cosine_sim = np.zeros((len(edge_list),1))\n",
    "    hub_prom = np.zeros((len(edge_list),1))\n",
    "    hub_depr = np.zeros((len(edge_list),1))\n",
    "    lhn = np.zeros((len(edge_list),1))\n",
    "    resource_all = np.zeros((len(edge_list),1))\n",
    "    local_path001 = np.zeros((len(edge_list),1))\n",
    "    local_path01 = np.zeros((len(edge_list),1))\n",
    "    local_path1 = np.zeros((len(edge_list),1))\n",
    "    #commute_time = np.zeros((len(edge_list),1))\n",
    "    #cosine_sim_time = np.zeros((len(edge_list),1))\n",
    "    #rooted_page001 = np.zeros((len(edge_list),1))\n",
    "    #rooted_page01 = np.zeros((len(edge_list),1))\n",
    "    #rooted_page1 = np.zeros((len(edge_list),1))\n",
    "\n",
    "    # Loop through edges to extract features\n",
    "    for edge in range(0,len(edge_list)):\n",
    "\n",
    "        # Shortest path\n",
    "        if nx.has_path(G, edge_list[edge][0], edge_list[edge][1]) == True:\n",
    "            shortest_path[edge] = len(nx.shortest_path(G, edge_list[edge][0], edge_list[edge][1]))-1\n",
    "        else:\n",
    "            shortest_path[edge] = 1000\n",
    "\n",
    "        # Common neighbors\n",
    "        common_neighbors[edge] = sum(1 for i in nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "\n",
    "        # Preferential attachment\n",
    "        pref_attach[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))*sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "\n",
    "        # Neighbor sum\n",
    "        neighbor_sum[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))+sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "\n",
    "        # Jaccard coefficient\n",
    "        temp = nx.jaccard_coefficient(G,[edge_list[edge]])\n",
    "        jaccard_coeff[edge] = list(temp)[0][2]\n",
    "\n",
    "        # Sorensen Index\n",
    "        sorensen[edge] = common_neighbors[edge]/neighbor_sum[edge]\n",
    "\n",
    "        # Cosine Similarity\n",
    "        cosine_sim[edge] = common_neighbors[edge]/np.sqrt(pref_attach[edge])\n",
    "\n",
    "        # Hub Promoted\n",
    "        hub_prom[edge] = common_neighbors[edge]/min(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "\n",
    "        # Hub Depressed\n",
    "        hub_depr[edge] = common_neighbors[edge]/max(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "\n",
    "        # LHN\n",
    "        lhn[edge] = common_neighbors[edge]/pref_attach[edge]\n",
    "\n",
    "        # Adamic/Adar\n",
    "        temp = nx.adamic_adar_index(G,[edge_list[edge]])\n",
    "        try: adamic_adar[edge] = list(temp)[0][2]\n",
    "        except ZeroDivisionError: adamic_adar[edge] = 1000\n",
    "\n",
    "        # Resource Allocation\n",
    "        temp = list(nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "        temp = dict(G.degree(temp))\n",
    "        resource_all[edge] = sum(1/i for i in list(temp.values()))\n",
    "\n",
    "        # Clustering coefficient\n",
    "        local_cluster_sum[edge] = cluster_coeff[edge_list[edge][0]] + cluster_coeff[edge_list[edge][1]]\n",
    "        local_cluster_prod[edge] = cluster_coeff[edge_list[edge][0]] * cluster_coeff[edge_list[edge][1]]\n",
    "\n",
    "        # Local Path\n",
    "        local_path001[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .001*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "        local_path01[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .01*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "        local_path1[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .1*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "\n",
    "        # Commute Time\n",
    "        #temp = (Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1] + Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1] - 2*Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1])\n",
    "        #commute_time[edge] = len(edge_list)*temp\n",
    "\n",
    "        # Cosine similarity\n",
    "        #temp = np.sqrt(Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1]*Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1])\n",
    "        #cosine_sim_time[edge] = Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1] / temp\n",
    "\n",
    "        # Rooted pagerank\n",
    "        #rooted_page001[edge] = RPR001[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "        #rooted_page01[edge] = RPR01[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "        #rooted_page1[edge] = RPR1[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "\n",
    "        ####### Meta Data Attributes\n",
    "\n",
    "        # Same gender\n",
    "        same_gender[edge] = (node_attr.loc[edge_list[edge][0]-1,'gender'] + node_attr.loc[edge_list[edge][1]-1,'gender'])%2\n",
    "\n",
    "        # Same status\n",
    "        if node_attr.loc[edge_list[edge][0]-1,'status'] == node_attr.loc[edge_list[edge][1]-1,'status']:\n",
    "            same_status[edge] = 1\n",
    "        else:\n",
    "            same_status[edge] = 0\n",
    "\n",
    "        # Same major\n",
    "        if node_attr.loc[edge_list[edge][0]-1,'major'] == node_attr.loc[edge_list[edge][1]-1,'major']:\n",
    "            same_major[edge] = 1\n",
    "        else:\n",
    "            same_major[edge] = 0\n",
    "\n",
    "        # Same dorm\n",
    "        if node_attr.loc[edge_list[edge][0]-1,'dorm'] == node_attr.loc[edge_list[edge][1]-1,'dorm']:\n",
    "            same_dorm[edge] = 1\n",
    "        else:\n",
    "            same_dorm[edge] = 0\n",
    "\n",
    "        # Same year\n",
    "        if node_attr.loc[edge_list[edge][0]-1,'year'] == node_attr.loc[edge_list[edge][1]-1,'year']:\n",
    "            same_year[edge] = 1\n",
    "        else:\n",
    "            same_year[edge] = 0\n",
    "\n",
    "        ####### Print statement for tracking\n",
    "        if edge%10000 == 0:\n",
    "            print(str(np.round(edge/len(edge_list)*100,1)) + '%' , end = \" \")\n",
    "        \n",
    "    \"\"\"\n",
    "    Create data frame with all of the features\n",
    "    \"\"\"\n",
    "    # Initialize data frame to store features\n",
    "    feature_df = pd.DataFrame.from_records(edge_list, columns = ['Node_1', 'Node_2'])\n",
    "\n",
    "    # Create features for data frame\n",
    "    feature_df['shortest_path'] = shortest_path\n",
    "    feature_df['common_neighbors'] = common_neighbors \n",
    "    feature_df['pref_attach'] = pref_attach \n",
    "    feature_df['neighbor_sum'] = neighbor_sum \n",
    "    feature_df['sorensen'] = sorensen \n",
    "    feature_df['cosine_sim'] = cosine_sim \n",
    "    feature_df['hub_prom'] = hub_prom \n",
    "    feature_df['hub_depr'] = hub_depr \n",
    "    feature_df['lhn'] = lhn\n",
    "    feature_df['adamic_adar'] = adamic_adar\n",
    "    feature_df['resource_all'] = resource_all \n",
    "    feature_df['local_cluster_sum'] = local_cluster_sum\n",
    "    feature_df['local_cluster_prod'] = local_cluster_prod\n",
    "    feature_df['same_gender'] = same_gender\n",
    "    feature_df['same_status'] = same_status\n",
    "    feature_df['same_major'] = same_major\n",
    "    feature_df['same_dorm'] = same_dorm\n",
    "    feature_df['same_year'] = same_year\n",
    "    feature_df['local_path001'] = local_path001\n",
    "    feature_df['local_path01'] = local_path01\n",
    "    feature_df['local_path1'] = local_path1\n",
    "    #feature_df['commute_time'] = commute_time\n",
    "    #feature_df['cosine_sim_time'] = cosine_sim_time\n",
    "    #feature_df['rooted_page001'] = rooted_page001\n",
    "    #feature_df['rooted_page01'] = rooted_page01\n",
    "    #feature_df['rooted_page1'] = rooted_page1\n",
    "    feature_df['edge'] = y\n",
    "    feature_df['label'] = label\n",
    "    \n",
    "    \"\"\"\n",
    "    Save features to a data frame\n",
    "    \"\"\"\n",
    "    # Feature data frame\n",
    "    feature_df.to_csv(filename + '.csv')\n",
    "            \n",
    "    \"\"\"\n",
    "    Print statement for tracking\n",
    "    \"\"\"\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
