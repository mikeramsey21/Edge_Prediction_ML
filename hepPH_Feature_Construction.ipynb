{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Michael E. Ramsey\n",
    "CSCI 5352\n",
    "Date Created: 11/01/18\n",
    "Last Edited: 12/3/18\n",
    "\n",
    "This is a python script to analyze the citation network, hepPH, presented in CSCI 5352.\n",
    "In this file, we compute features for each edge pair and a randomly sampled set of the \n",
    "non-edge pairs. \n",
    "\n",
    "The output of this scipt is a csv, containing several \"features\" of each edge and non-edge pair.\n",
    "This .csv file will be used to create machine learning models for edge prediction.\n",
    "\n",
    "You can get the data at the following link:\n",
    "https://icon.colorado.edu/\n",
    "\"\"\"\n",
    "\n",
    "# Get necessary libraries\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy.sparse import csr_matrix\n",
    "from random import randint\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract and load the data\n",
    "\"\"\"\n",
    "\n",
    "# Get list of filenames that contain edge information\n",
    "# Had to exclude a bunch of files that I did not need\n",
    "# Could have done this more efficiently\n",
    "filepath = \"hepPH_Edges/\"\n",
    "filename = \"hepPH_edges.csv\"\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(filepath + filename)\n",
    "\n",
    "# Delete the first column\n",
    "data = data.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FromNodeID</th>\n",
       "      <th>ToNodeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9903282</td>\n",
       "      <td>9211202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9601388</td>\n",
       "      <td>9211202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9905409</td>\n",
       "      <td>9211202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9702367</td>\n",
       "      <td>9211202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9912370</td>\n",
       "      <td>9211202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FromNodeID  ToNodeID\n",
       "0     9903282   9211202\n",
       "1     9601388   9211202\n",
       "2     9905409   9211202\n",
       "3     9702367   9211202\n",
       "4     9912370   9211202"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "View the data\n",
    "\"\"\"\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract node and edge list\n",
    "\"\"\"\n",
    "# Extract the edge list\n",
    "edge_list = list(zip(data['FromNodeID'], data['ToNodeID']))\n",
    "edge_list = list(set(edge_list))\n",
    "\n",
    "# Extract node list\n",
    "node_list = list(set(data['FromNodeID']) | set(data['ToNodeID']))\n",
    "\n",
    "# Create list of not_edges - adjust for proportion of edges that exist in the network\n",
    "total_val = int(np.round(len(edge_list)*(1+len(edge_list)/(len(node_list)-1)**2)))\n",
    "not_edges = [(node_list[randint(0, len(node_list)-1)], node_list[randint(0, len(node_list)-1)]) for _ in range(total_val)]\n",
    "not_edges = list(set(not_edges) - set(edge_list))\n",
    "\n",
    "# Create label vectors\n",
    "y_edges = np.ones((len(edge_list),1))\n",
    "y_not_edges = np.zeros((len(not_edges),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform train/valid/test split\n",
    "We will use 95/2.5/2.5\n",
    "\"\"\"\n",
    "edge_train, edge_test, y_train, y_test = train_test_split(edge_list, y_edges, test_size = .7, random_state = 345)\n",
    "not_edge_train, not_edge_test, y_not_train, y_not_test = train_test_split(not_edges, y_not_edges, test_size = .7, random_state = 345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate the network\n",
    "\"\"\"\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_edges_from(edge_train)\n",
    "\n",
    "# Compute clustering coefficient for each ndoe\n",
    "cluster_coeff = nx.clustering(G)\n",
    "\n",
    "# Construct adjacency matrices and multiples\n",
    "A = nx.adjacency_matrix(G)\n",
    "Asquared = A.dot(A)\n",
    "Acubed = Asquared.dot(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% 1.4% 2.9% 4.3% 5.8% 7.2% 8.6% 10.1% 11.5% 13.0% 14.4% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8% 17.3% 18.7% 20.2% 21.6% 23.0% 24.5% 25.9% 27.4% 28.8% 30.2% 31.7% 33.1% 34.6% 36.0% 37.4% 38.9% 40.3% 41.8% 43.2% 44.6% 46.1% 47.5% 49.0% 50.4% 51.8% 53.3% 54.7% 56.2% 57.6% 59.0% 60.5% 61.9% 63.4% 64.8% 66.2% 67.7% 69.1% 70.6% 72.0% 73.4% 74.9% 76.3% "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Concatenate lists\n",
    "\"\"\"\n",
    "edge_list = edge_train + not_edge_train + edge_test + not_edge_test\n",
    "y = np.concatenate((y_train, y_not_train, y_test, y_not_test))\n",
    "label = ['Tr']*(len(edge_train)+len(not_edge_train)) + ['T']*(len(edge_test)+len(not_edge_test))\n",
    "\n",
    "\"\"\"\n",
    "Feature Construction\n",
    "\"\"\"\n",
    "# Initialize numpy arrays to store features\n",
    "#shortest_path = np.zeros((len(edge_list),1))\n",
    "common_neighbors = np.zeros((len(edge_list),1))\n",
    "pref_attach = np.zeros((len(edge_list),1))\n",
    "neighbor_sum = np.zeros((len(edge_list),1))\n",
    "local_cluster_sum = np.zeros((len(edge_list),1))\n",
    "local_cluster_prod = np.zeros((len(edge_list),1))\n",
    "jaccard_coeff = np.zeros((len(edge_list),1))\n",
    "adamic_adar = np.zeros((len(edge_list),1))\n",
    "sorensen = np.zeros((len(edge_list),1))\n",
    "cosine_sim = np.zeros((len(edge_list),1))\n",
    "hub_prom = np.zeros((len(edge_list),1))\n",
    "hub_depr = np.zeros((len(edge_list),1))\n",
    "lhn = np.zeros((len(edge_list),1))\n",
    "resource_all = np.zeros((len(edge_list),1))\n",
    "local_path001 = np.zeros((len(edge_list),1))\n",
    "local_path01 = np.zeros((len(edge_list),1))\n",
    "local_path1 = np.zeros((len(edge_list),1))\n",
    "#commute_time = np.zeros((len(edge_list),1))\n",
    "#cosine_sim_time = np.zeros((len(edge_list),1))\n",
    "#rooted_page001 = np.zeros((len(edge_list),1))\n",
    "#rooted_page01 = np.zeros((len(edge_list),1))\n",
    "#rooted_page1 = np.zeros((len(edge_list),1))\n",
    "\n",
    "# Loop through edges to extract features\n",
    "for edge in range(0,len(edge_list)):\n",
    "\n",
    "    # Shortest path\n",
    "    #if nx.has_path(G, edge_list[edge][0], edge_list[edge][1]) == True:\n",
    "    #    shortest_path[edge] = len(nx.shortest_path(G, edge_list[edge][0], edge_list[edge][1]))-1\n",
    "    #else:\n",
    "    #    shortest_path[edge] = 1000\n",
    "\n",
    "    # Common neighbors\n",
    "    common_neighbors[edge] = sum(1 for i in nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "\n",
    "    # Preferential attachment\n",
    "    pref_attach[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))*sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "\n",
    "    # Neighbor sum\n",
    "    neighbor_sum[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))+sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "\n",
    "    # Jaccard coefficient\n",
    "    temp = nx.jaccard_coefficient(G,[edge_list[edge]])\n",
    "    jaccard_coeff[edge] = list(temp)[0][2]\n",
    "\n",
    "    # Sorensen Index\n",
    "    sorensen[edge] = common_neighbors[edge]/neighbor_sum[edge]\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim[edge] = common_neighbors[edge]/np.sqrt(pref_attach[edge])\n",
    "\n",
    "    # Hub Promoted\n",
    "    hub_prom[edge] = common_neighbors[edge]/min(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "\n",
    "    # Hub Depressed\n",
    "    hub_depr[edge] = common_neighbors[edge]/max(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "\n",
    "    # LHN\n",
    "    lhn[edge] = common_neighbors[edge]/pref_attach[edge]\n",
    "\n",
    "    # Adamic/Adar\n",
    "    temp = nx.adamic_adar_index(G,[edge_list[edge]])\n",
    "    try: adamic_adar[edge] = list(temp)[0][2]\n",
    "    except ZeroDivisionError: adamic_adar[edge] = 1000\n",
    "\n",
    "    # Resource Allocation\n",
    "    temp = list(nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "    temp = dict(G.degree(temp))\n",
    "    resource_all[edge] = sum(1/i for i in list(temp.values()))\n",
    "\n",
    "    # Clustering coefficient\n",
    "    local_cluster_sum[edge] = cluster_coeff[edge_list[edge][0]] + cluster_coeff[edge_list[edge][1]]\n",
    "    local_cluster_prod[edge] = cluster_coeff[edge_list[edge][0]] * cluster_coeff[edge_list[edge][1]]\n",
    "\n",
    "    # Local Path\n",
    "    node1 = node_list.index(edge_list[edge][0])\n",
    "    node2 = node_list.index(edge_list[edge][1])\n",
    "    local_path001[edge] = Asquared[node1,node2] + .001*Acubed[node1,node2]\n",
    "    local_path01[edge] = Asquared[node1,node2] + .01*Acubed[node1,node2]\n",
    "    local_path1[edge] = Asquared[node1,node2] + .1*Acubed[node1,node2]\n",
    "\n",
    "    # Commute Time\n",
    "    #temp = (Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1] + Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1] - 2*Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1])\n",
    "    #commute_time[edge] = len(edge_list)*temp\n",
    "\n",
    "    # Cosine similarity\n",
    "    #temp = np.sqrt(Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1]*Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1])\n",
    "    #cosine_sim_time[edge] = Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1] / temp\n",
    "\n",
    "    # Rooted pagerank\n",
    "    #rooted_page001[edge] = RPR001[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    #rooted_page01[edge] = RPR01[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    #rooted_page1[edge] = RPR1[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "\n",
    "    ####### Print statement for tracking\n",
    "    if edge%10000 == 0:\n",
    "        print(str(np.round(edge/len(edge_list)*100,1)) + '%' , end = \" \")\n",
    "\n",
    "\"\"\"\n",
    "Create data frame with all of the features\n",
    "\"\"\"\n",
    "# Initialize data frame to store features\n",
    "feature_df = pd.DataFrame.from_records(edge_list, columns = ['Node_1', 'Node_2'])\n",
    "\n",
    "# Create features for data frame\n",
    "#feature_df['shortest_path'] = shortest_path\n",
    "feature_df['common_neighbors'] = common_neighbors \n",
    "feature_df['pref_attach'] = pref_attach \n",
    "feature_df['neighbor_sum'] = neighbor_sum \n",
    "feature_df['sorensen'] = sorensen \n",
    "feature_df['cosine_sim'] = cosine_sim \n",
    "feature_df['hub_prom'] = hub_prom \n",
    "feature_df['hub_depr'] = hub_depr \n",
    "feature_df['lhn'] = lhn\n",
    "feature_df['adamic_adar'] = adamic_adar\n",
    "feature_df['resource_all'] = resource_all \n",
    "feature_df['local_cluster_sum'] = local_cluster_sum\n",
    "feature_df['local_cluster_prod'] = local_cluster_prod\n",
    "feature_df['local_path001'] = local_path001\n",
    "feature_df['local_path01'] = local_path01\n",
    "feature_df['local_path1'] = local_path1\n",
    "#feature_df['commute_time'] = commute_time\n",
    "#feature_df['cosine_sim_time'] = cosine_sim_time\n",
    "#feature_df['rooted_page001'] = rooted_page001\n",
    "#feature_df['rooted_page01'] = rooted_page01\n",
    "#feature_df['rooted_page1'] = rooted_page1\n",
    "feature_df['edge'] = y\n",
    "feature_df['label'] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save features to a data frame\n",
    "\"\"\"\n",
    "# Feature data frame\n",
    "feature_df.to_csv('hepPH_features_30' + '.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
