{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Michael E. Ramsey\n",
    "CSCI 5352\n",
    "Date Created: 11/01/18\n",
    "Last Edited: 12/3/18\n",
    "\n",
    "This is a python script to analyze the enron email network presented in CSCI 5352.\n",
    "In this file, we compute features for each edge pair and a randomly sampled set of the \n",
    "non-edge pairs. \n",
    "\n",
    "The output of this scipt is a csv, containing several \"features\" of each edge and non-edge pair.\n",
    "This .csv file will be used to create machine learning models for edge prediction.\n",
    "\"\"\"\n",
    "\n",
    "# Get necessary libraries\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy.sparse import csr_matrix\n",
    "from random import randint\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract all filenames for the enron pairs\n",
    "\"\"\"\n",
    "\n",
    "# Get list of filenames that contain edge information\n",
    "# Had to exclude a bunch of files that I did not need\n",
    "# Could have done this more efficiently\n",
    "filepath = \"Enron_Networks-Months/\"\n",
    "files = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the edges for all networks\n",
    "\"\"\"\n",
    "network_list = []\n",
    "for filename in files:\n",
    "    \n",
    "    edge_list = []\n",
    "    # Open file and read lines\n",
    "    f = open(filepath + filename)\n",
    "    f1 = f.readlines()\n",
    "    f.close()\n",
    "  \n",
    "    # Loop through lines and record edge\n",
    "    for x in f1:\n",
    "        temp = x.replace(\"\\t\",\" \")\n",
    "        edge_list.append(tuple(map(int, temp.split())))\n",
    "    network_list.append(edge_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Construct data frame for edges\n",
    "\"\"\"\n",
    "enron_df = pd.DataFrame(columns = ['Node_1', 'Node_2', 'Month', 'Year', 'Counter'])\n",
    "month = 5\n",
    "year = 1999\n",
    "counter = 1\n",
    "for network in network_list:\n",
    "    temp = pd.DataFrame.from_records(network, columns = ['Node_1', 'Node_2'])\n",
    "    temp['Month'] = [month]*len(network)\n",
    "    temp['Year'] = [year]*len(network)\n",
    "    temp['Counter'] = [counter]*len(network)\n",
    "    month += 1\n",
    "    counter += 1\n",
    "    enron_df = enron_df.append(temp)\n",
    "    if month == 13:\n",
    "        month = 1 \n",
    "        year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We train up to 2001: predict communications for 2002\n",
    "\"\"\"\n",
    "# Label training data and testing data\n",
    "enron_df.loc[(enron_df['Year'] <= 2002) & (enron_df['Month'] < 11), 'label'] = 'Tr'\n",
    "enron_df.loc[(enron_df['Year'] >= 2002) & (enron_df['Month'] >= 11), 'label'] = 'T'\n",
    "\n",
    "# Extract training and testing data\n",
    "train = enron_df.loc[enron_df['label'] == 'Tr']\n",
    "test = enron_df.loc[enron_df['label'] == 'T']\n",
    "\n",
    "# Extract the edge list\n",
    "edge_list = list(zip(enron_df['Node_1'], enron_df['Node_2']))\n",
    "edge_list = list(set(edge_list))\n",
    "edge_train = list(zip(train['Node_1'], train['Node_2']))\n",
    "edge_train = list(set(edge_train))\n",
    "edge_test = list(zip(test['Node_1'], test['Node_2']))\n",
    "edge_test = list(set(edge_test) - set(edge_train))\n",
    "\n",
    "# Get the list of nodes\n",
    "node_list = list(range(1,150+1))\n",
    "\n",
    "# Create list of not_edges - adjust for proportion of edges that exist in the network\n",
    "total_val = int(np.round(len(edge_train)*(1+len(edge_train)/(len(node_list)-1)**2)))\n",
    "not_edge_train = [(randint(1, len(node_list)), randint(1, len(node_list))) for _ in range(total_val)]\n",
    "not_edge_train = list(set(not_edge_train) - set(edge_train) - set(edge_test))\n",
    "total_val2 = int(np.round(len(edge_test)*(1+len(edge_test)/(len(node_list)-1)**2)))\n",
    "not_edge_test = [(randint(1, len(node_list)), randint(1, len(node_list))) for _ in range(total_val2)]\n",
    "not_edge_test = list(set(not_edge_test) - set(edge_train) - set(edge_test) - set(not_edge_train))\n",
    "\n",
    "# Create label vectors\n",
    "y_train = np.ones((len(edge_train),1))\n",
    "y_test = np.ones((len(edge_test),1))\n",
    "y_not_train = np.zeros((len(not_edge_train),1))\n",
    "y_not_test = np.zeros((len(not_edge_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Concatenate lists\n",
    "\"\"\"\n",
    "edge_list = edge_train + not_edge_train + edge_test + not_edge_test\n",
    "y = np.concatenate((y_train, y_not_train, y_test, y_not_test))\n",
    "label = ['Tr']*(len(edge_train)+len(not_edge_train)) + ['T']*(len(edge_test)+len(not_edge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate the network\n",
    "\"\"\"\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_edges_from(edge_train)\n",
    "\n",
    "# Compute clustering coefficient for each ndoe\n",
    "cluster_coeff = nx.clustering(G)\n",
    "\n",
    "# Construct adjacency matrices and multiples\n",
    "A = nx.adjacency_matrix(G)\n",
    "Asquared = A.dot(A)\n",
    "Acubed = Asquared.dot(A)\n",
    "\n",
    "# Construct row sum sum diagonal matrix\n",
    "#D = (A.todense().sum(axis = 1)).flatten()\n",
    "#D = csr_matrix(np.diag(np.array(D)[0]))\n",
    "#Dinv = np.diag(1/np.diag(D.todense()))\n",
    "\n",
    "# Coompute pseudo-inverse\n",
    "#L = D-A\n",
    "#Lstar = np.linalg.pinv(L.todense())\n",
    "\n",
    "# Compute rooted pagerank\n",
    "#RPR001 = (1-.001)*np.linalg.inv(np.identity(A.shape[0]) - .001*(Dinv.dot(A.todense())))\n",
    "#RPR01 = (1-.01)*np.linalg.inv(np.identity(A.shape[0]) - .01*(Dinv.dot(A.todense())))\n",
    "#RPR1 = (1-.1)*np.linalg.inv(np.identity(A.shape[0]) - .1*(Dinv.dot(A.todense())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\mikee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Construction\n",
    "\"\"\"\n",
    "# Initialize numpy arrays to store features\n",
    "shortest_path = np.zeros((len(edge_list),1))\n",
    "common_neighbors = np.zeros((len(edge_list),1))\n",
    "pref_attach = np.zeros((len(edge_list),1))\n",
    "neighbor_sum = np.zeros((len(edge_list),1))\n",
    "local_cluster_sum = np.zeros((len(edge_list),1))\n",
    "local_cluster_prod = np.zeros((len(edge_list),1))\n",
    "jaccard_coeff = np.zeros((len(edge_list),1))\n",
    "adamic_adar = np.zeros((len(edge_list),1))\n",
    "same_gender = np.zeros((len(edge_list),1))\n",
    "same_status = np.zeros((len(edge_list),1))\n",
    "same_major = np.zeros((len(edge_list),1))\n",
    "same_dorm = np.zeros((len(edge_list),1))\n",
    "same_year = np.zeros((len(edge_list),1))\n",
    "sorensen = np.zeros((len(edge_list),1))\n",
    "cosine_sim = np.zeros((len(edge_list),1))\n",
    "hub_prom = np.zeros((len(edge_list),1))\n",
    "hub_depr = np.zeros((len(edge_list),1))\n",
    "lhn = np.zeros((len(edge_list),1))\n",
    "resource_all = np.zeros((len(edge_list),1))\n",
    "local_path001 = np.zeros((len(edge_list),1))\n",
    "local_path01 = np.zeros((len(edge_list),1))\n",
    "local_path1 = np.zeros((len(edge_list),1))\n",
    "#commute_time = np.zeros((len(edge_list),1))\n",
    "#cosine_sim_time = np.zeros((len(edge_list),1))\n",
    "#rooted_page001 = np.zeros((len(edge_list),1))\n",
    "#rooted_page01 = np.zeros((len(edge_list),1))\n",
    "#rooted_page1 = np.zeros((len(edge_list),1))\n",
    "\n",
    "# Loop through edges to extract features\n",
    "for edge in range(0,len(edge_list)):\n",
    "\n",
    "    # Shortest path\n",
    "    if nx.has_path(G, edge_list[edge][0], edge_list[edge][1]) == True:\n",
    "        shortest_path[edge] = len(nx.shortest_path(G, edge_list[edge][0], edge_list[edge][1]))-1\n",
    "    else:\n",
    "        shortest_path[edge] = 1000\n",
    "\n",
    "    # Common neighbors\n",
    "    common_neighbors[edge] = sum(1 for i in nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "\n",
    "    # Preferential attachment\n",
    "    pref_attach[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))*sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "\n",
    "    # Neighbor sum\n",
    "    neighbor_sum[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))+sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "\n",
    "    # Jaccard coefficient\n",
    "    temp = nx.jaccard_coefficient(G,[edge_list[edge]])\n",
    "    jaccard_coeff[edge] = list(temp)[0][2]\n",
    "\n",
    "    # Sorensen Index\n",
    "    sorensen[edge] = common_neighbors[edge]/neighbor_sum[edge]\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim[edge] = common_neighbors[edge]/np.sqrt(pref_attach[edge])\n",
    "\n",
    "    # Hub Promoted\n",
    "    hub_prom[edge] = common_neighbors[edge]/min(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "\n",
    "    # Hub Depressed\n",
    "    hub_depr[edge] = common_neighbors[edge]/max(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "\n",
    "    # LHN\n",
    "    lhn[edge] = common_neighbors[edge]/pref_attach[edge]\n",
    "\n",
    "    # Adamic/Adar\n",
    "    temp = nx.adamic_adar_index(G,[edge_list[edge]])\n",
    "    try: adamic_adar[edge] = list(temp)[0][2]\n",
    "    except ZeroDivisionError: adamic_adar[edge] = 1000\n",
    "\n",
    "    # Resource Allocation\n",
    "    temp = list(nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "    temp = dict(G.degree(temp))\n",
    "    resource_all[edge] = sum(1/i for i in list(temp.values()))\n",
    "\n",
    "    # Clustering coefficient\n",
    "    local_cluster_sum[edge] = cluster_coeff[edge_list[edge][0]] + cluster_coeff[edge_list[edge][1]]\n",
    "    local_cluster_prod[edge] = cluster_coeff[edge_list[edge][0]] * cluster_coeff[edge_list[edge][1]]\n",
    "\n",
    "    # Local Path\n",
    "    local_path001[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .001*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    local_path01[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .01*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    local_path1[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .1*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "\n",
    "    # Commute Time\n",
    "    #temp = (Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1] + Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1] - 2*Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1])\n",
    "    #commute_time[edge] = len(edge_list)*temp\n",
    "\n",
    "    # Cosine similarity\n",
    "    #temp = np.sqrt(Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1]*Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1])\n",
    "    #cosine_sim_time[edge] = Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1] / temp\n",
    "\n",
    "    # Rooted pagerank\n",
    "    #rooted_page001[edge] = RPR001[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    #rooted_page01[edge] = RPR01[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    #rooted_page1[edge] = RPR1[edge_list[edge][0]-1,edge_list[edge][1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create data frame with all of the features\n",
    "\"\"\"\n",
    "# Initialize data frame to store features\n",
    "feature_df = pd.DataFrame.from_records(edge_list, columns = ['Node_1', 'Node_2'])\n",
    "\n",
    "# Create features for data frame\n",
    "feature_df['shortest_path'] = shortest_path\n",
    "feature_df['common_neighbors'] = common_neighbors\n",
    "feature_df['pref_attach'] = pref_attach\n",
    "feature_df['neighbor_sum'] = neighbor_sum\n",
    "feature_df['sorensen'] = sorensen\n",
    "feature_df['cosine_sim'] = cosine_sim\n",
    "feature_df['hub_prom'] = hub_prom\n",
    "feature_df['hub_depr'] = hub_depr\n",
    "feature_df['hub_prom'] = hub_prom\n",
    "feature_df['lhn'] = lhn\n",
    "feature_df['adamic_adar'] = adamic_adar\n",
    "feature_df['resource_all'] = resource_all\n",
    "feature_df['local_cluster_sum'] = local_cluster_sum\n",
    "feature_df['local_cluster_prod'] = local_cluster_prod\n",
    "feature_df['local_path001'] = local_path001\n",
    "feature_df['local_path01'] = local_path01\n",
    "feature_df['local_path1'] = local_path1\n",
    "#feature_df['commute_time'] = commute_time\n",
    "#feature_df['cosine_sim_time'] = cosine_sim_time\n",
    "#feature_df['rooted_page001'] = rooted_page001\n",
    "#feature_df['rooted_page01'] = rooted_page01\n",
    "#feature_df['rooted_page1'] = rooted_page1\n",
    "feature_df['edge'] = y\n",
    "feature_df['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save features to a data frame\n",
    "\"\"\"\n",
    "# Feature data frame\n",
    "feature_df.to_csv('Enron_2002_10' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
