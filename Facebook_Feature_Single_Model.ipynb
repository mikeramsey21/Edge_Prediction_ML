{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike Ramsey\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Michael E. Ramsey\n",
    "CSCI 5352\n",
    "Date Created: 11/01/18\n",
    "Last Edited: 11/20/18\n",
    "\n",
    "This is a python script to analyze the Network of Facebook data presented in CSCI 5352.\n",
    "In this file, we compute features for each edge pair and a randomly sampled set of the \n",
    "non-edge pairs. \n",
    "\n",
    "The output of this scipt is a csv, containing several \"features\" of each edge and non-edge pair.\n",
    "This .csv file will be used to create machine learning models for edge prediction.\n",
    "\"\"\"\n",
    "\n",
    "# Get necessary libraries\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract all filenames for facebook100 dataset\n",
    "\"\"\"\n",
    "\n",
    "# Get list of filenames that contain edge information\n",
    "# Had to exclude a bunch of files that I did not need\n",
    "# Could have done this more efficiently\n",
    "files = [f for f in listdir(\"../Data/facebook100txt/\") if isfile(join(\"../Data/facebook100txt/\", f)) \n",
    "         if not(f.endswith('r.txt')) if f.endswith('.txt') if not(f.endswith('readme_aaron.txt'))\n",
    "         if not(f.endswith('facebook100_readme_021011.txt'))]\n",
    "len(files)\n",
    "# Got all 100 Yes!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the nodes and edges.\n",
    "\"\"\"\n",
    "\n",
    "# Choose the dataset to import\n",
    "filename = 'American75'\n",
    "\n",
    "# Create empty list to store connections\n",
    "edge_list = []\n",
    "\n",
    "# Open file and read lines\n",
    "f = open(\"../Data/facebook100txt/\" + filename + '.txt')\n",
    "f1 = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Loop through lines and record edge\n",
    "for x in f1:\n",
    "    temp = x[:-1].replace(\"\\t\",\" \")\n",
    "    edge_list.append(tuple(map(int, temp.split())))\n",
    "\n",
    "# Create a flattened list and find unique elements to create node list\n",
    "flat_list = [item for sublist in edge_list for item in sublist]\n",
    "node_list = set(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2044"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform train/valid/test split\n",
    "We will use 95/2.5/2.5\n",
    "\"\"\"\n",
    "edge_list, edge_test = train_test_split(edge_list, test_size = .01, random_state = 345)\n",
    "edge_valid, edge_test = train_test_split(edge_test, test_size = .5, random_state = 345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract Node Attributes\n",
    "\"\"\"\n",
    "# Extract node attributes\n",
    "node_attr = pd.read_table(\"../Data/facebook100txt/\" + filename + '_attr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate the network\n",
    "\"\"\"\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "# Compute clustering coefficient for each ndoe\n",
    "cluster_coeff = nx.clustering(G)\n",
    "\n",
    "# Construct adjacency matrices and multiples\n",
    "A = nx.adjacency_matrix(G)\n",
    "Asquared = A.dot(A)\n",
    "Acubed = Asquared.dot(A)\n",
    "\n",
    "# Construct row sum sum diagonal matrix\n",
    "D = (A.todense().sum(axis = 1)).flatten()\n",
    "D = csr_matrix(np.diag(np.array(D)[0]))\n",
    "#Dinv = np.diag(1/np.diag(D.todense()))\n",
    "\n",
    "# Compute pseudo-inverse\n",
    "L = D-A\n",
    "Lstar = np.linalg.pinv(L.todense())\n",
    "\n",
    "# Compute rooted pagerank\n",
    "#RPR001 = (1-.001)*np.linalg.inv(np.identity(A.shape[0]) - .001*(Dinv.dot(A.todense())))\n",
    "#RPR01 = (1-.01)*np.linalg.inv(np.identity(A.shape[0]) - .01*(Dinv.dot(A.todense())))\n",
    "#RPR1 = (1-.1)*np.linalg.inv(np.identity(A.shape[0]) - .1*(Dinv.dot(A.todense())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000 20000 30000 40000 "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Construction\n",
    "\"\"\"\n",
    "\n",
    "# Initialize numpy arrays to store features\n",
    "shortest_path = np.zeros((len(edge_list),1))\n",
    "common_neighbors = np.zeros((len(edge_list),1))\n",
    "pref_attach = np.zeros((len(edge_list),1))\n",
    "neighbor_sum = np.zeros((len(edge_list),1))\n",
    "local_cluster_sum = np.zeros((len(edge_list),1))\n",
    "local_cluster_prod = np.zeros((len(edge_list),1))\n",
    "jaccard_coeff = np.zeros((len(edge_list),1))\n",
    "adamic_adar = np.zeros((len(edge_list),1))\n",
    "same_gender = np.zeros((len(edge_list),1))\n",
    "same_status = np.zeros((len(edge_list),1))\n",
    "same_major = np.zeros((len(edge_list),1))\n",
    "same_dorm = np.zeros((len(edge_list),1))\n",
    "same_year = np.zeros((len(edge_list),1))\n",
    "sorensen = np.zeros((len(edge_list),1))\n",
    "cosine_sim = np.zeros((len(edge_list),1))\n",
    "hub_prom = np.zeros((len(edge_list),1))\n",
    "hub_depr = np.zeros((len(edge_list),1))\n",
    "lhn = np.zeros((len(edge_list),1))\n",
    "resource_all = np.zeros((len(edge_list),1))\n",
    "local_path001 = np.zeros((len(edge_list),1))\n",
    "local_path01 = np.zeros((len(edge_list),1))\n",
    "local_path1 = np.zeros((len(edge_list),1))\n",
    "commute_time = np.zeros((len(edge_list),1))\n",
    "cosine_sim_time = np.zeros((len(edge_list),1))\n",
    "#rooted_page001 = np.zeros((len(edge_list),1))\n",
    "#rooted_page01 = np.zeros((len(edge_list),1))\n",
    "#rooted_page1 = np.zeros((len(edge_list),1))\n",
    "\n",
    "# Loop through edges to extract features\n",
    "for edge in range(0,len(edge_list)):\n",
    "    \n",
    "    # Shortest path\n",
    "    shortest_path[edge] = len(nx.shortest_path(G, edge_list[edge][0], edge_list[edge][1]))-1\n",
    "    \n",
    "    # Common neighbors\n",
    "    common_neighbors[edge] = sum(1 for i in nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "    \n",
    "    # Preferential attachment\n",
    "    pref_attach[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))*sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "    \n",
    "    # Neighbor sum\n",
    "    neighbor_sum[edge] = sum(1 for i in G.neighbors(edge_list[edge][0]))+sum(1 for i in G.neighbors(edge_list[edge][1]))\n",
    "    \n",
    "    # Jaccard coefficient\n",
    "    temp = nx.jaccard_coefficient(G,[edge_list[edge]])\n",
    "    jaccard_coeff[edge] = list(temp)[0][2]\n",
    "    \n",
    "    # Sorensen Index\n",
    "    sorensen[edge] = common_neighbors[edge]/neighbor_sum[edge]\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    cosine_sim[edge] = common_neighbors[edge]/np.sqrt(pref_attach[edge])\n",
    "    \n",
    "    # Hub Promoted\n",
    "    hub_prom[edge] = common_neighbors[edge]/min(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "    \n",
    "    # Hub Depressed\n",
    "    hub_depr[edge] = common_neighbors[edge]/max(sum(1 for i in G.neighbors(edge_list[edge][0])), sum(1 for i in G.neighbors(edge_list[edge][1])))\n",
    "    \n",
    "    # LHN\n",
    "    lhn[edge] = common_neighbors[edge]/pref_attach[edge]\n",
    "    \n",
    "    # Adamic/Adar\n",
    "    temp = nx.adamic_adar_index(G,[edge_list[edge]])\n",
    "    adamic_adar[edge] = list(temp)[0][2]\n",
    "    \n",
    "    # Resource Allocation\n",
    "    temp = list(nx.common_neighbors(G, edge_list[edge][0], edge_list[edge][1]))\n",
    "    temp = dict(G.degree(temp))\n",
    "    resource_all[edge] = sum(1/i for i in list(temp.values()))\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    local_cluster_sum[edge] = cluster_coeff[edge_list[edge][0]] + cluster_coeff[edge_list[edge][1]]\n",
    "    local_cluster_prod[edge] = cluster_coeff[edge_list[edge][0]] * cluster_coeff[edge_list[edge][1]]\n",
    "    \n",
    "    # Local Path\n",
    "    local_path001[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .001*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    local_path01[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .01*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    local_path1[edge] = Asquared[edge_list[edge][0]-1,edge_list[edge][1]-1] + .1*Acubed[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    \n",
    "    # Commute Time\n",
    "    temp = (Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1] + Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1] - 2*Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1])\n",
    "    commute_time[edge] = len(edge_list)*temp\n",
    "    \n",
    "    # Cosine similarity\n",
    "    temp = np.sqrt(Lstar[edge_list[edge][0]-1, edge_list[edge][0]-1]*Lstar[edge_list[edge][1]-1, edge_list[edge][1]-1])\n",
    "    cosine_sim_time[edge] = Lstar[edge_list[edge][0]-1, edge_list[edge][1]-1] / temp\n",
    "    \n",
    "    # Rooted pagerank\n",
    "    #rooted_page001[edge] = RPR001[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    #rooted_page01[edge] = RPR01[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    #rooted_page1[edge] = RPR1[edge_list[edge][0]-1,edge_list[edge][1]-1]\n",
    "    \n",
    "    ####### Meta Data Attributes\n",
    "    \n",
    "    # Same gender\n",
    "    same_gender[edge] = (node_attr.loc[edge_list[edge][0]-1,'gender'] + node_attr.loc[edge_list[edge][1]-1,'gender'])%2\n",
    "    \n",
    "    # Same status\n",
    "    if node_attr.loc[edge_list[edge][0]-1,'status'] == node_attr.loc[edge_list[edge][1]-1,'status']:\n",
    "        same_status[edge] = 1\n",
    "    else:\n",
    "        same_status[edge] = 0\n",
    "    \n",
    "    # Same major\n",
    "    if node_attr.loc[edge_list[edge][0]-1,'major'] == node_attr.loc[edge_list[edge][1]-1,'major']:\n",
    "        same_major[edge] = 1\n",
    "    else:\n",
    "        same_major[edge] = 0\n",
    "        \n",
    "    # Same dorm\n",
    "    if node_attr.loc[edge_list[edge][0]-1,'dorm'] == node_attr.loc[edge_list[edge][1]-1,'dorm']:\n",
    "        same_dorm[edge] = 1\n",
    "    else:\n",
    "        same_dorm[edge] = 0\n",
    "        \n",
    "    # Same year\n",
    "    if node_attr.loc[edge_list[edge][0]-1,'year'] == node_attr.loc[edge_list[edge][1]-1,'year']:\n",
    "        same_year[edge] = 1\n",
    "    else:\n",
    "        same_year[edge] = 0\n",
    "    \n",
    "    ####### Print statement for tracking\n",
    "    if edge%10000 == 0:\n",
    "        print(edge, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create data frame with all of the features\n",
    "\"\"\"\n",
    "\n",
    "# Initialize data frame to store features\n",
    "feature_df = pd.DataFrame.from_records(edge_list, columns = ['Node_1', 'Node_2'])\n",
    "\n",
    "# Create features for data frame\n",
    "feature_df['shortest_path'] = shortest_path\n",
    "feature_df['common_neighbors'] = common_neighbors\n",
    "feature_df['pref_attach'] = pref_attach\n",
    "feature_df['neighbor_sum'] = neighbor_sum\n",
    "feature_df['sorensen'] = sorensen\n",
    "feature_df['cosine_sim'] = cosine_sim\n",
    "feature_df['hub_prom'] = hub_prom\n",
    "feature_df['hub_depr'] = hub_depr\n",
    "feature_df['hub_prom'] = hub_prom\n",
    "feature_df['lhn'] = lhn\n",
    "feature_df['adamic_adar'] = adamic_adar\n",
    "feature_df['resource_all'] = resource_all\n",
    "feature_df['local_cluster_sum'] = local_cluster_sum\n",
    "feature_df['local_cluster_prod'] = local_cluster_prod\n",
    "feature_df['same_gender'] = same_gender\n",
    "feature_df['same_status'] = same_status\n",
    "feature_df['same_major'] = same_major\n",
    "feature_df['same_dorm'] = same_dorm\n",
    "feature_df['same_year'] = same_year\n",
    "feature_df['local_path001'] = local_path001\n",
    "feature_df['local_path01'] = local_path01\n",
    "feature_df['local_path1'] = local_path1\n",
    "feature_df['commute_time'] = commute_time\n",
    "feature_df['cosine_sim_time'] = cosine_sim_time\n",
    "#feature_df['rooted_page001'] = rooted_page001\n",
    "#feature_df['rooted_page01'] = rooted_page01\n",
    "#feature_df['rooted_page1'] = rooted_page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save features to a data frame\n",
    "\"\"\"\n",
    "# Feature data frame\n",
    "feature_df.to_csv(filename + '.csv')\n",
    "\n",
    "# Valid and test sets to text files\n",
    "with open(filename + '_valid.txt', 'w') as f:\n",
    "    for item in edge_valid:\n",
    "        f.write(str(item[0]) +  \" \" + str(item[1]) + '\\n')\n",
    "with open(filename + '_test.txt', 'w') as f:\n",
    "    for item in edge_test:\n",
    "        f.write(str(item[0]) + \" \" + str(item[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
